{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tq7aJx-Z2HW4"
   },
   "source": [
    "# Task 1 Kuzushiji Kanji Classification\n",
    "- 61070278\n",
    "- 61070306\n",
    "- 61070365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\shine_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\user\\anaconda3\\envs\\shine_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\user\\anaconda3\\envs\\shine_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\user\\anaconda3\\envs\\shine_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\user\\anaconda3\\envs\\shine_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\user\\anaconda3\\envs\\shine_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\user\\anaconda3\\envs\\shine_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\user\\anaconda3\\envs\\shine_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\user\\anaconda3\\envs\\shine_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\user\\anaconda3\\envs\\shine_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\user\\anaconda3\\envs\\shine_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\user\\anaconda3\\envs\\shine_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout, Input, UpSampling2D\n",
    "from tensorflow.python.keras.layers import MaxPooling2D, BatchNormalization\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score#, confusion_matrix\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เช็คว่ามี GPU มั้ย เพราะถ้าใช้ GPU ในการคำนวณจะเร็วกว่าการใช้ CPU มากๆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17587244745636157317\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3069915955\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 3349367025809373174\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtt4S0mHGhRl"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_13pvcak8Md7",
    "outputId": "c8980f18-8374-4f1b-a672-5d2d7faac50d"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# X = np.load('/content/drive/MyDrive/Colab Notebooks/kuzushiji-ml-it-kmitl-2020/train-images.npy')\n",
    "# y = np.load('/content/drive/MyDrive/Colab Notebooks/kuzushiji-ml-it-kmitl-2020/train-labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vKmTxUmemqZ4"
   },
   "outputs": [],
   "source": [
    "X = np.load('train-images.npy')\n",
    "y = np.load('train-labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y3T2DYmisf0b",
    "outputId": "a4c64731-6beb-4e15-b2d1-a32238c70a42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101376, 64, 64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzJ8or9VKcL0"
   },
   "source": [
    "## Plot Some Samples of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "l0bYMinNKaBk",
    "outputId": "01b27996-5fb4-4110-aa85-156cded1d570"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2e7a2b6a048>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "figsize = (5,5)\n",
    "plt.figure(figsize=figsize)\n",
    "plt.axis('off')\n",
    "plt.title(y[0])\n",
    "plt.imshow(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-GaEk6ztAPP"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "sSv-iP53C4EA"
   },
   "outputs": [],
   "source": [
    "# ทำการ reshape ข้อมูล ปรับให้เป็น 4096 เพราะ 64*64\n",
    "X_ = X.reshape(-1, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255 0\n"
     ]
    }
   ],
   "source": [
    "# เช็คค่ามากที่สุดและน้อยที่สุดเพื่อจะทำการปรับ scale ใหม่\n",
    "print(X.max(), X.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ปรับ scale ข้อมูลเพื่อให้คอมพิวเตอร์ทำงานได้เร็วขึ้น"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1B5_SdHaXxou"
   },
   "outputs": [],
   "source": [
    "# ทำการปรับ scale ใหม่ให้อยู่ระหว่าง 0-1\n",
    "X_ = X_ / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51UdKLQRC6Rd",
    "outputId": "22993863-3094-45f1-8b60-d6f8bb02be30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013615821117607585"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_X = normalize(X_)\n",
    "np.std(normalized_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KxKjFkE7dmFS",
    "outputId": "bdcaa98c-cd89-4f00-e2bb-0c08f158e427"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101376, 64, 64, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ทำการ reshape ให้เหมาะสมกับอัลกอริทึม\n",
    "X_norm = normalized_X.reshape(X_.shape[0], 64, 64, 1)\n",
    "X_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pppuxnoHJv3n",
    "outputId": "bc30885d-dc5a-41b8-9f8c-60d02c18ea24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91238, 64, 64, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ทำการแบ่งแยกข้อมูลเป็นส่วน train และ validation \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_norm, y, test_size=0.1, random_state=42, stratify=y)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1*X GB of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=(1024*4))])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBj9J3PlTz8s"
   },
   "source": [
    "## Define Model Architecture\n",
    "ออกแบบ nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1IqPLvyxlLtd",
    "outputId": "cd122f0d-e51d-4fd5-97f0-7ba317762778"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1080 (64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "IMG_ROWS = X_train.shape[1]\n",
    "IMG_COLS = X_train.shape[2]\n",
    "NUM_CLASSES = len(np.unique(y_train))\n",
    "INPUT_SHAPE = (IMG_ROWS, IMG_COLS, 1)\n",
    "\n",
    "print(NUM_CLASSES, INPUT_SHAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackabuse.com/image-recognition-in-python-with-tensorflow-and-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- activation จะเลือกใช้เป็น relu เพราะเป็นที่นิยมและมีคุณภาพ\n",
    "- padding กำหนดให้เป็น same ซึ่งหมายถึงเราจะไม่เปลี่ยนขนาดของภาพ\n",
    "- MaxPooling2D ลดขนาดอินพุต\n",
    "- ใส่ dropout เพื่อไม่ให้โมเดล overfitting\n",
    "- Batch Normalization จะทำให้อินพุตที่มุ่งหน้าไปยังเลเยอร์ถัดไปเป็นปกติ เพื่อให้แน่ใจว่าเครือข่ายจะมีการกระจายเดียวกันกับที่เราต้องการเสมอ\n",
    "- Flatten เลเยอร์สุดท้ายของ CNN ต้องการให้ข้อมูลอยู่ในรูปของเวกเตอร์เพื่อประมวลผล ด้วยเหตุนี้ข้อมูลจึงต้องใช้ flatten เพื่อที่ค่าจะได้ถูกบีบอัดเป็นเวกเตอร์\n",
    "- Dense อันสุดท้ายจะกำหนดให้มีจำนวนเท่ากับ Label ของเรา"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "6nGH7QCZj-U6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\envs\\shine_env\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=INPUT_SHAPE, padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc']\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ehnEk-NwO6Uq",
    "outputId": "4359835b-10ef-4796-b2f0-09bcb0cfb510"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 64, 64, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1080)              277560    \n",
      "=================================================================\n",
      "Total params: 3,033,016\n",
      "Trainable params: 3,029,496\n",
      "Non-trainable params: 3,520\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display the model's architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "fMaZbVgU1ADw"
   },
   "outputs": [],
   "source": [
    "# checkpoint_filepath = '/content/drive/MyDrive/Colab Notebooks/kuzushiji-ml-it-kmitl-2020/model/cnn_weights_5.h5'\n",
    "checkpoint_filepath = 'model/cnn_weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "sWEXDDsBxdOX"
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=checkpoint_filepath, verbose=1, \n",
    "                               monitor='val_loss',save_best_only=True, mode='min')\n",
    "\n",
    "earlystoper = EarlyStopping(monitor='val_loss', patience=30) #ถ้าไม่ดีขึ้นใน 30 รอบให้หยุด"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 82114 samples, validate on 9124 samples\n",
      "Epoch 1/100\n",
      "82080/82114 [============================>.] - ETA: 0s - loss: 3.2027 - acc: 0.3985\n",
      "Epoch 00001: val_loss improved from inf to 1.44981, saving model to model/cnn_weights_5.h5\n",
      "82114/82114 [==============================] - 53s 646us/sample - loss: 3.2022 - acc: 0.3986 - val_loss: 1.4498 - val_acc: 0.6619\n",
      "Epoch 2/100\n",
      "82048/82114 [============================>.] - ETA: 0s - loss: 1.4428 - acc: 0.6577\n",
      "Epoch 00002: val_loss improved from 1.44981 to 0.72851, saving model to model/cnn_weights_5.h5\n",
      "82114/82114 [==============================] - 49s 594us/sample - loss: 1.4425 - acc: 0.6578 - val_loss: 0.7285 - val_acc: 0.8196\n",
      "Epoch 3/100\n",
      "82048/82114 [============================>.] - ETA: 0s - loss: 1.0057 - acc: 0.7471\n",
      "Epoch 00003: val_loss improved from 0.72851 to 0.59142, saving model to model/cnn_weights_5.h5\n",
      "82114/82114 [==============================] - 47s 566us/sample - loss: 1.0056 - acc: 0.7471 - val_loss: 0.5914 - val_acc: 0.8541\n",
      "Epoch 4/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.8256 - acc: 0.7848\n",
      "Epoch 00004: val_loss improved from 0.59142 to 0.51101, saving model to model/cnn_weights_5.h5\n",
      "82114/82114 [==============================] - 49s 594us/sample - loss: 0.8258 - acc: 0.7848 - val_loss: 0.5110 - val_acc: 0.8757\n",
      "Epoch 5/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.7016 - acc: 0.8143\n",
      "Epoch 00005: val_loss improved from 0.51101 to 0.42576, saving model to model/cnn_weights_5.h5\n",
      "82114/82114 [==============================] - 47s 576us/sample - loss: 0.7016 - acc: 0.8142 - val_loss: 0.4258 - val_acc: 0.8973\n",
      "Epoch 6/100\n",
      "82048/82114 [============================>.] - ETA: 0s - loss: 0.6188 - acc: 0.8327\n",
      "Epoch 00006: val_loss improved from 0.42576 to 0.38669, saving model to model/cnn_weights_5.h5\n",
      "82114/82114 [==============================] - 46s 562us/sample - loss: 0.6188 - acc: 0.8328 - val_loss: 0.3867 - val_acc: 0.9072\n",
      "Epoch 7/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.5499 - acc: 0.8494\n",
      "Epoch 00007: val_loss did not improve from 0.38669\n",
      "82114/82114 [==============================] - 50s 606us/sample - loss: 0.5499 - acc: 0.8494 - val_loss: 0.8196 - val_acc: 0.8058\n",
      "Epoch 8/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.5365 - acc: 0.8530\n",
      "Epoch 00008: val_loss improved from 0.38669 to 0.34570, saving model to model/cnn_weights_5.h5\n",
      "82114/82114 [==============================] - 50s 603us/sample - loss: 0.5365 - acc: 0.8530 - val_loss: 0.3457 - val_acc: 0.9208\n",
      "Epoch 9/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.4625 - acc: 0.8708\n",
      "Epoch 00009: val_loss did not improve from 0.34570\n",
      "82114/82114 [==============================] - 48s 589us/sample - loss: 0.4625 - acc: 0.8708 - val_loss: 0.3502 - val_acc: 0.9217\n",
      "Epoch 10/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.4352 - acc: 0.8777\n",
      "Epoch 00010: val_loss improved from 0.34570 to 0.32557, saving model to model/cnn_weights_5.h5\n",
      "82114/82114 [==============================] - 49s 594us/sample - loss: 0.4353 - acc: 0.8777 - val_loss: 0.3256 - val_acc: 0.9268\n",
      "Epoch 11/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.4198 - acc: 0.8805\n",
      "Epoch 00011: val_loss did not improve from 0.32557\n",
      "82114/82114 [==============================] - 49s 595us/sample - loss: 0.4200 - acc: 0.8805 - val_loss: 0.3452 - val_acc: 0.9187\n",
      "Epoch 12/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.4018 - acc: 0.8866\n",
      "Epoch 00012: val_loss improved from 0.32557 to 0.31719, saving model to model/cnn_weights_5.h5\n",
      "82114/82114 [==============================] - 49s 597us/sample - loss: 0.4020 - acc: 0.8866 - val_loss: 0.3172 - val_acc: 0.9284\n",
      "Epoch 13/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.3751 - acc: 0.8919\n",
      "Epoch 00013: val_loss improved from 0.31719 to 0.30372, saving model to model/cnn_weights_5.h5\n",
      "82114/82114 [==============================] - 50s 606us/sample - loss: 0.3751 - acc: 0.8919 - val_loss: 0.3037 - val_acc: 0.9333\n",
      "Epoch 14/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.3492 - acc: 0.8989\n",
      "Epoch 00014: val_loss did not improve from 0.30372\n",
      "82114/82114 [==============================] - 49s 593us/sample - loss: 0.3493 - acc: 0.8989 - val_loss: 0.3419 - val_acc: 0.9205\n",
      "Epoch 15/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.3472 - acc: 0.9008\n",
      "Epoch 00015: val_loss did not improve from 0.30372\n",
      "82114/82114 [==============================] - 49s 600us/sample - loss: 0.3473 - acc: 0.9008 - val_loss: 0.3091 - val_acc: 0.9329\n",
      "Epoch 16/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.3766 - acc: 0.8917\n",
      "Epoch 00016: val_loss did not improve from 0.30372\n",
      "82114/82114 [==============================] - 49s 598us/sample - loss: 0.3767 - acc: 0.8917 - val_loss: 0.3272 - val_acc: 0.9301\n",
      "Epoch 17/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.3207 - acc: 0.9066\n",
      "Epoch 00017: val_loss improved from 0.30372 to 0.29843, saving model to model/cnn_weights_5.h5\n",
      "82114/82114 [==============================] - 50s 605us/sample - loss: 0.3208 - acc: 0.9066 - val_loss: 0.2984 - val_acc: 0.9372\n",
      "Epoch 18/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.3110 - acc: 0.9081\n",
      "Epoch 00018: val_loss improved from 0.29843 to 0.29659, saving model to model/cnn_weights_5.h5\n",
      "82114/82114 [==============================] - 49s 596us/sample - loss: 0.3111 - acc: 0.9081 - val_loss: 0.2966 - val_acc: 0.9372\n",
      "Epoch 19/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.2858 - acc: 0.9161\n",
      "Epoch 00019: val_loss improved from 0.29659 to 0.29244, saving model to model/cnn_weights_5.h5\n",
      "82114/82114 [==============================] - 49s 596us/sample - loss: 0.2858 - acc: 0.9161 - val_loss: 0.2924 - val_acc: 0.9388\n",
      "Epoch 20/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.2788 - acc: 0.9171\n",
      "Epoch 00020: val_loss did not improve from 0.29244\n",
      "82114/82114 [==============================] - 49s 597us/sample - loss: 0.2789 - acc: 0.9171 - val_loss: 0.2988 - val_acc: 0.9385\n",
      "Epoch 21/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.2773 - acc: 0.9178\n",
      "Epoch 00021: val_loss did not improve from 0.29244\n",
      "82114/82114 [==============================] - 48s 590us/sample - loss: 0.2773 - acc: 0.9178 - val_loss: 0.3034 - val_acc: 0.9351\n",
      "Epoch 22/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.2641 - acc: 0.9218\n",
      "Epoch 00022: val_loss did not improve from 0.29244\n",
      "82114/82114 [==============================] - 50s 606us/sample - loss: 0.2642 - acc: 0.9218 - val_loss: 0.3133 - val_acc: 0.9383\n",
      "Epoch 23/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.2917 - acc: 0.9142\n",
      "Epoch 00023: val_loss improved from 0.29244 to 0.28857, saving model to model/cnn_weights_5.h5\n",
      "82114/82114 [==============================] - 48s 579us/sample - loss: 0.2918 - acc: 0.9142 - val_loss: 0.2886 - val_acc: 0.9406\n",
      "Epoch 24/100\n",
      "82048/82114 [============================>.] - ETA: 0s - loss: 0.2361 - acc: 0.9283\n",
      "Epoch 00024: val_loss improved from 0.28857 to 0.28633, saving model to model/cnn_weights_5.h5\n",
      "82114/82114 [==============================] - 48s 589us/sample - loss: 0.2362 - acc: 0.9283 - val_loss: 0.2863 - val_acc: 0.9420\n",
      "Epoch 25/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.2327 - acc: 0.9297\n",
      "Epoch 00025: val_loss did not improve from 0.28633\n",
      "82114/82114 [==============================] - 49s 597us/sample - loss: 0.2329 - acc: 0.9297 - val_loss: 0.2923 - val_acc: 0.9421\n",
      "Epoch 26/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.2306 - acc: 0.9314\n",
      "Epoch 00026: val_loss did not improve from 0.28633\n",
      "82114/82114 [==============================] - 49s 594us/sample - loss: 0.2308 - acc: 0.9314 - val_loss: 0.3069 - val_acc: 0.9397\n",
      "Epoch 27/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.2604 - acc: 0.9231- ETA: 1s - los\n",
      "Epoch 00027: val_loss did not improve from 0.28633\n",
      "82114/82114 [==============================] - 48s 579us/sample - loss: 0.2605 - acc: 0.9230 - val_loss: 0.2907 - val_acc: 0.9430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.2177 - acc: 0.9345\n",
      "Epoch 00028: val_loss improved from 0.28633 to 0.27809, saving model to model/cnn_weights_5.h5\n",
      "82114/82114 [==============================] - 50s 611us/sample - loss: 0.2177 - acc: 0.9345 - val_loss: 0.2781 - val_acc: 0.9443\n",
      "Epoch 29/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.2065 - acc: 0.9375\n",
      "Epoch 00029: val_loss did not improve from 0.27809\n",
      "82114/82114 [==============================] - 50s 605us/sample - loss: 0.2066 - acc: 0.9374 - val_loss: 0.2818 - val_acc: 0.9433\n",
      "Epoch 30/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.2141 - acc: 0.9347\n",
      "Epoch 00030: val_loss did not improve from 0.27809\n",
      "82114/82114 [==============================] - 49s 595us/sample - loss: 0.2142 - acc: 0.9347 - val_loss: 0.2845 - val_acc: 0.9434\n",
      "Epoch 31/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.2097 - acc: 0.9358\n",
      "Epoch 00031: val_loss did not improve from 0.27809\n",
      "82114/82114 [==============================] - 50s 611us/sample - loss: 0.2097 - acc: 0.9358 - val_loss: 0.2964 - val_acc: 0.9394\n",
      "Epoch 32/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.2032 - acc: 0.9386\n",
      "Epoch 00032: val_loss did not improve from 0.27809\n",
      "82114/82114 [==============================] - 49s 597us/sample - loss: 0.2033 - acc: 0.9386 - val_loss: 0.2903 - val_acc: 0.9421\n",
      "Epoch 33/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.2017 - acc: 0.9385\n",
      "Epoch 00033: val_loss did not improve from 0.27809\n",
      "82114/82114 [==============================] - 49s 596us/sample - loss: 0.2017 - acc: 0.9385 - val_loss: 0.3102 - val_acc: 0.9400\n",
      "Epoch 34/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.2535 - acc: 0.9238\n",
      "Epoch 00034: val_loss did not improve from 0.27809\n",
      "82114/82114 [==============================] - 49s 591us/sample - loss: 0.2536 - acc: 0.9238 - val_loss: 0.3331 - val_acc: 0.9339\n",
      "Epoch 35/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.2262 - acc: 0.9312\n",
      "Epoch 00035: val_loss did not improve from 0.27809\n",
      "82114/82114 [==============================] - 50s 612us/sample - loss: 0.2263 - acc: 0.9312 - val_loss: 0.3081 - val_acc: 0.9397\n",
      "Epoch 36/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.2264 - acc: 0.9321\n",
      "Epoch 00036: val_loss did not improve from 0.27809\n",
      "82114/82114 [==============================] - 48s 590us/sample - loss: 0.2264 - acc: 0.9321 - val_loss: 0.3016 - val_acc: 0.9426\n",
      "Epoch 37/100\n",
      "82048/82114 [============================>.] - ETA: 0s - loss: 0.1841 - acc: 0.9438\n",
      "Epoch 00037: val_loss did not improve from 0.27809\n",
      "82114/82114 [==============================] - 46s 565us/sample - loss: 0.1841 - acc: 0.9438 - val_loss: 0.2825 - val_acc: 0.9453\n",
      "Epoch 38/100\n",
      "82080/82114 [============================>.] - ETA: 0s - loss: 0.1740 - acc: 0.9457\n",
      "Epoch 00038: val_loss did not improve from 0.27809\n",
      "82114/82114 [==============================] - 46s 561us/sample - loss: 0.1740 - acc: 0.9458 - val_loss: 0.2864 - val_acc: 0.9450\n",
      "Epoch 39/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1854 - acc: 0.9430\n",
      "Epoch 00039: val_loss did not improve from 0.27809\n",
      "82114/82114 [==============================] - 46s 563us/sample - loss: 0.1855 - acc: 0.9430 - val_loss: 0.2813 - val_acc: 0.9455\n",
      "Epoch 40/100\n",
      "82080/82114 [============================>.] - ETA: 0s - loss: 0.2214 - acc: 0.9329\n",
      "Epoch 00040: val_loss did not improve from 0.27809\n",
      "82114/82114 [==============================] - 46s 564us/sample - loss: 0.2216 - acc: 0.9329 - val_loss: 0.2924 - val_acc: 0.9439\n",
      "Epoch 41/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9434\n",
      "Epoch 00041: val_loss improved from 0.27809 to 0.27807, saving model to model/cnn_weights_5.h5\n",
      "82114/82114 [==============================] - 46s 563us/sample - loss: 0.1838 - acc: 0.9433 - val_loss: 0.2781 - val_acc: 0.9476\n",
      "Epoch 42/100\n",
      "82048/82114 [============================>.] - ETA: 0s - loss: 0.1651 - acc: 0.9498\n",
      "Epoch 00042: val_loss improved from 0.27807 to 0.27589, saving model to model/cnn_weights_5.h5\n",
      "82114/82114 [==============================] - 46s 564us/sample - loss: 0.1650 - acc: 0.9498 - val_loss: 0.2759 - val_acc: 0.9482\n",
      "Epoch 43/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1898 - acc: 0.9412\n",
      "Epoch 00043: val_loss did not improve from 0.27589\n",
      "82114/82114 [==============================] - 46s 562us/sample - loss: 0.1898 - acc: 0.9412 - val_loss: 0.2820 - val_acc: 0.9472\n",
      "Epoch 44/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1608 - acc: 0.9496\n",
      "Epoch 00044: val_loss did not improve from 0.27589\n",
      "82114/82114 [==============================] - 46s 560us/sample - loss: 0.1610 - acc: 0.9495 - val_loss: 0.2844 - val_acc: 0.9472\n",
      "Epoch 45/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1994 - acc: 0.9389\n",
      "Epoch 00045: val_loss did not improve from 0.27589\n",
      "82114/82114 [==============================] - 46s 562us/sample - loss: 0.1994 - acc: 0.9389 - val_loss: 0.3080 - val_acc: 0.9413\n",
      "Epoch 46/100\n",
      "82080/82114 [============================>.] - ETA: 0s - loss: 0.1622 - acc: 0.9506\n",
      "Epoch 00046: val_loss did not improve from 0.27589\n",
      "82114/82114 [==============================] - 46s 565us/sample - loss: 0.1621 - acc: 0.9506 - val_loss: 0.2938 - val_acc: 0.9445\n",
      "Epoch 47/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1543 - acc: 0.9523\n",
      "Epoch 00047: val_loss did not improve from 0.27589\n",
      "82114/82114 [==============================] - 47s 568us/sample - loss: 0.1544 - acc: 0.9522 - val_loss: 0.2869 - val_acc: 0.9452\n",
      "Epoch 48/100\n",
      "82048/82114 [============================>.] - ETA: 0s - loss: 0.1576 - acc: 0.9518\n",
      "Epoch 00048: val_loss improved from 0.27589 to 0.27181, saving model to model/cnn_weights_5.h5\n",
      "82114/82114 [==============================] - 47s 566us/sample - loss: 0.1579 - acc: 0.9517 - val_loss: 0.2718 - val_acc: 0.9479\n",
      "Epoch 49/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1764 - acc: 0.9453\n",
      "Epoch 00049: val_loss did not improve from 0.27181\n",
      "82114/82114 [==============================] - 46s 562us/sample - loss: 0.1766 - acc: 0.9453 - val_loss: 0.2903 - val_acc: 0.9451\n",
      "Epoch 50/100\n",
      "82048/82114 [============================>.] - ETA: 0s - loss: 0.1525 - acc: 0.9526\n",
      "Epoch 00050: val_loss did not improve from 0.27181\n",
      "82114/82114 [==============================] - 46s 563us/sample - loss: 0.1526 - acc: 0.9526 - val_loss: 0.2833 - val_acc: 0.9460\n",
      "Epoch 51/100\n",
      "82080/82114 [============================>.] - ETA: 0s - loss: 0.1460 - acc: 0.9550\n",
      "Epoch 00051: val_loss did not improve from 0.27181\n",
      "82114/82114 [==============================] - 46s 564us/sample - loss: 0.1459 - acc: 0.9551 - val_loss: 0.2814 - val_acc: 0.9479\n",
      "Epoch 52/100\n",
      "82048/82114 [============================>.] - ETA: 0s - loss: 0.1459 - acc: 0.9546\n",
      "Epoch 00052: val_loss did not improve from 0.27181\n",
      "82114/82114 [==============================] - 46s 561us/sample - loss: 0.1459 - acc: 0.9546 - val_loss: 0.2825 - val_acc: 0.9483\n",
      "Epoch 53/100\n",
      "82048/82114 [============================>.] - ETA: 0s - loss: 0.1485 - acc: 0.9530\n",
      "Epoch 00053: val_loss did not improve from 0.27181\n",
      "82114/82114 [==============================] - 46s 564us/sample - loss: 0.1485 - acc: 0.9530 - val_loss: 0.2899 - val_acc: 0.9456\n",
      "Epoch 54/100\n",
      "82048/82114 [============================>.] - ETA: 0s - loss: 0.1488 - acc: 0.9528\n",
      "Epoch 00054: val_loss did not improve from 0.27181\n",
      "82114/82114 [==============================] - 46s 563us/sample - loss: 0.1490 - acc: 0.9527 - val_loss: 0.2887 - val_acc: 0.9486\n",
      "Epoch 55/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1378 - acc: 0.9575\n",
      "Epoch 00055: val_loss did not improve from 0.27181\n",
      "82114/82114 [==============================] - 46s 560us/sample - loss: 0.1378 - acc: 0.9575 - val_loss: 0.2869 - val_acc: 0.9449\n",
      "Epoch 56/100\n",
      "82080/82114 [============================>.] - ETA: 0s - loss: 0.1402 - acc: 0.9566\n",
      "Epoch 00056: val_loss did not improve from 0.27181\n",
      "82114/82114 [==============================] - 46s 560us/sample - loss: 0.1403 - acc: 0.9566 - val_loss: 0.2885 - val_acc: 0.9467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1429 - acc: 0.9554\n",
      "Epoch 00057: val_loss did not improve from 0.27181\n",
      "82114/82114 [==============================] - 46s 563us/sample - loss: 0.1430 - acc: 0.9554 - val_loss: 0.2829 - val_acc: 0.9473\n",
      "Epoch 58/100\n",
      "82080/82114 [============================>.] - ETA: 0s - loss: 0.1484 - acc: 0.9541\n",
      "Epoch 00058: val_loss did not improve from 0.27181\n",
      "82114/82114 [==============================] - 46s 561us/sample - loss: 0.1484 - acc: 0.9541 - val_loss: 0.2768 - val_acc: 0.9509\n",
      "Epoch 59/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1292 - acc: 0.9596\n",
      "Epoch 00059: val_loss did not improve from 0.27181\n",
      "82114/82114 [==============================] - 46s 563us/sample - loss: 0.1293 - acc: 0.9596 - val_loss: 0.2748 - val_acc: 0.9518\n",
      "Epoch 60/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1286 - acc: 0.9596\n",
      "Epoch 00060: val_loss did not improve from 0.27181\n",
      "82114/82114 [==============================] - 46s 562us/sample - loss: 0.1287 - acc: 0.9596 - val_loss: 0.2840 - val_acc: 0.9493\n",
      "Epoch 61/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9579\n",
      "Epoch 00061: val_loss did not improve from 0.27181\n",
      "82114/82114 [==============================] - 46s 562us/sample - loss: 0.1342 - acc: 0.9579 - val_loss: 0.2803 - val_acc: 0.9508\n",
      "Epoch 62/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1250 - acc: 0.9603\n",
      "Epoch 00062: val_loss did not improve from 0.27181\n",
      "82114/82114 [==============================] - 46s 560us/sample - loss: 0.1252 - acc: 0.9603 - val_loss: 0.2791 - val_acc: 0.9510\n",
      "Epoch 63/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1553 - acc: 0.9525\n",
      "Epoch 00063: val_loss did not improve from 0.27181\n",
      "82114/82114 [==============================] - 46s 564us/sample - loss: 0.1553 - acc: 0.9525 - val_loss: 0.2931 - val_acc: 0.9486\n",
      "Epoch 64/100\n",
      "82048/82114 [============================>.] - ETA: 0s - loss: 0.1312 - acc: 0.9584\n",
      "Epoch 00064: val_loss did not improve from 0.27181\n",
      "82114/82114 [==============================] - 46s 561us/sample - loss: 0.1314 - acc: 0.9584 - val_loss: 0.2771 - val_acc: 0.9508\n",
      "Epoch 65/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1209 - acc: 0.9618\n",
      "Epoch 00065: val_loss did not improve from 0.27181\n",
      "82114/82114 [==============================] - 46s 562us/sample - loss: 0.1209 - acc: 0.9618 - val_loss: 0.2780 - val_acc: 0.9506\n",
      "Epoch 66/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1141 - acc: 0.9633\n",
      "Epoch 00066: val_loss did not improve from 0.27181\n",
      "82114/82114 [==============================] - 46s 561us/sample - loss: 0.1142 - acc: 0.9632 - val_loss: 0.2745 - val_acc: 0.9506\n",
      "Epoch 67/100\n",
      "82080/82114 [============================>.] - ETA: 0s - loss: 0.1209 - acc: 0.9618\n",
      "Epoch 00067: val_loss did not improve from 0.27181\n",
      "82114/82114 [==============================] - 46s 561us/sample - loss: 0.1209 - acc: 0.9618 - val_loss: 0.2817 - val_acc: 0.9496\n",
      "Epoch 68/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1200 - acc: 0.9618\n",
      "Epoch 00068: val_loss did not improve from 0.27181\n",
      "82114/82114 [==============================] - 46s 560us/sample - loss: 0.1202 - acc: 0.9618 - val_loss: 0.2789 - val_acc: 0.9509\n",
      "Epoch 69/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1237 - acc: 0.9609\n",
      "Epoch 00069: val_loss did not improve from 0.27181\n",
      "82114/82114 [==============================] - 46s 561us/sample - loss: 0.1238 - acc: 0.9609 - val_loss: 0.2800 - val_acc: 0.9493\n",
      "Epoch 70/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.9617\n",
      "Epoch 00070: val_loss did not improve from 0.27181\n",
      "82114/82114 [==============================] - 46s 560us/sample - loss: 0.1219 - acc: 0.9617 - val_loss: 0.2820 - val_acc: 0.9504\n",
      "Epoch 71/100\n",
      "82048/82114 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9630\n",
      "Epoch 00071: val_loss did not improve from 0.27181\n",
      "82114/82114 [==============================] - 46s 563us/sample - loss: 0.1162 - acc: 0.9629 - val_loss: 0.2872 - val_acc: 0.9480\n",
      "Epoch 72/100\n",
      "82048/82114 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9612\n",
      "Epoch 00072: val_loss did not improve from 0.27181\n",
      "82114/82114 [==============================] - 46s 561us/sample - loss: 0.1235 - acc: 0.9612 - val_loss: 0.2779 - val_acc: 0.9520\n",
      "Epoch 73/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1113 - acc: 0.9652\n",
      "Epoch 00073: val_loss did not improve from 0.27181\n",
      "82114/82114 [==============================] - 46s 563us/sample - loss: 0.1114 - acc: 0.9651 - val_loss: 0.2824 - val_acc: 0.9530\n",
      "Epoch 74/100\n",
      "82080/82114 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9630\n",
      "Epoch 00074: val_loss did not improve from 0.27181\n",
      "82114/82114 [==============================] - 46s 560us/sample - loss: 0.1192 - acc: 0.9630 - val_loss: 0.2804 - val_acc: 0.9510\n",
      "Epoch 75/100\n",
      "82048/82114 [============================>.] - ETA: 0s - loss: 0.1334 - acc: 0.9585\n",
      "Epoch 00075: val_loss did not improve from 0.27181\n",
      "82114/82114 [==============================] - 46s 560us/sample - loss: 0.1333 - acc: 0.9585 - val_loss: 0.2885 - val_acc: 0.9500\n",
      "Epoch 76/100\n",
      "82048/82114 [============================>.] - ETA: 0s - loss: 0.1208 - acc: 0.9620\n",
      "Epoch 00076: val_loss did not improve from 0.27181\n",
      "82114/82114 [==============================] - 46s 559us/sample - loss: 0.1209 - acc: 0.9620 - val_loss: 0.2859 - val_acc: 0.9495\n",
      "Epoch 77/100\n",
      "82048/82114 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9639\n",
      "Epoch 00077: val_loss improved from 0.27181 to 0.27077, saving model to model/cnn_weights_5.h5\n",
      "82114/82114 [==============================] - 46s 563us/sample - loss: 0.1155 - acc: 0.9639 - val_loss: 0.2708 - val_acc: 0.9523\n",
      "Epoch 78/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1144 - acc: 0.9636\n",
      "Epoch 00078: val_loss did not improve from 0.27077\n",
      "82114/82114 [==============================] - 46s 561us/sample - loss: 0.1144 - acc: 0.9636 - val_loss: 0.2815 - val_acc: 0.9509\n",
      "Epoch 79/100\n",
      "82048/82114 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.9578\n",
      "Epoch 00079: val_loss did not improve from 0.27077\n",
      "82114/82114 [==============================] - 46s 562us/sample - loss: 0.1350 - acc: 0.9578 - val_loss: 0.2857 - val_acc: 0.9502\n",
      "Epoch 80/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.9641\n",
      "Epoch 00080: val_loss did not improve from 0.27077\n",
      "82114/82114 [==============================] - 46s 559us/sample - loss: 0.1131 - acc: 0.9640 - val_loss: 0.2847 - val_acc: 0.9522\n",
      "Epoch 81/100\n",
      "82080/82114 [============================>.] - ETA: 0s - loss: 0.1063 - acc: 0.9667\n",
      "Epoch 00081: val_loss did not improve from 0.27077\n",
      "82114/82114 [==============================] - 46s 560us/sample - loss: 0.1063 - acc: 0.9667 - val_loss: 0.2805 - val_acc: 0.9516\n",
      "Epoch 82/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1197 - acc: 0.9628\n",
      "Epoch 00082: val_loss improved from 0.27077 to 0.27010, saving model to model/cnn_weights_5.h5\n",
      "82114/82114 [==============================] - 46s 561us/sample - loss: 0.1199 - acc: 0.9628 - val_loss: 0.2701 - val_acc: 0.9521\n",
      "Epoch 83/100\n",
      "82080/82114 [============================>.] - ETA: 0s - loss: 0.1046 - acc: 0.9668\n",
      "Epoch 00083: val_loss did not improve from 0.27010\n",
      "82114/82114 [==============================] - 46s 561us/sample - loss: 0.1047 - acc: 0.9668 - val_loss: 0.2714 - val_acc: 0.9535\n",
      "Epoch 84/100\n",
      "82048/82114 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9666\n",
      "Epoch 00084: val_loss did not improve from 0.27010\n",
      "82114/82114 [==============================] - 46s 560us/sample - loss: 0.1067 - acc: 0.9666 - val_loss: 0.2813 - val_acc: 0.9502\n",
      "Epoch 85/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1028 - acc: 0.9679\n",
      "Epoch 00085: val_loss did not improve from 0.27010\n",
      "82114/82114 [==============================] - 46s 560us/sample - loss: 0.1028 - acc: 0.9679 - val_loss: 0.2746 - val_acc: 0.9533\n",
      "Epoch 86/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9670\n",
      "Epoch 00086: val_loss did not improve from 0.27010\n",
      "82114/82114 [==============================] - 46s 561us/sample - loss: 0.1049 - acc: 0.9670 - val_loss: 0.2887 - val_acc: 0.9516\n",
      "Epoch 87/100\n",
      "82048/82114 [============================>.] - ETA: 0s - loss: 0.1069 - acc: 0.9659\n",
      "Epoch 00087: val_loss did not improve from 0.27010\n",
      "82114/82114 [==============================] - 46s 562us/sample - loss: 0.1068 - acc: 0.9659 - val_loss: 0.2772 - val_acc: 0.9534\n",
      "Epoch 88/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.9570\n",
      "Epoch 00088: val_loss did not improve from 0.27010\n",
      "82114/82114 [==============================] - 46s 560us/sample - loss: 0.1377 - acc: 0.9570 - val_loss: 0.2727 - val_acc: 0.9535\n",
      "Epoch 89/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9666\n",
      "Epoch 00089: val_loss did not improve from 0.27010\n",
      "82114/82114 [==============================] - 46s 562us/sample - loss: 0.1065 - acc: 0.9666 - val_loss: 0.2735 - val_acc: 0.9539\n",
      "Epoch 90/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1113 - acc: 0.9645\n",
      "Epoch 00090: val_loss did not improve from 0.27010\n",
      "82114/82114 [==============================] - 46s 561us/sample - loss: 0.1114 - acc: 0.9644 - val_loss: 0.2929 - val_acc: 0.9530\n",
      "Epoch 91/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9677\n",
      "Epoch 00091: val_loss did not improve from 0.27010\n",
      "82114/82114 [==============================] - 46s 562us/sample - loss: 0.1027 - acc: 0.9677 - val_loss: 0.2744 - val_acc: 0.9540\n",
      "Epoch 92/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9696\n",
      "Epoch 00092: val_loss did not improve from 0.27010\n",
      "82114/82114 [==============================] - 46s 561us/sample - loss: 0.0961 - acc: 0.9696 - val_loss: 0.2787 - val_acc: 0.9535\n",
      "Epoch 93/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.9696\n",
      "Epoch 00093: val_loss did not improve from 0.27010\n",
      "82114/82114 [==============================] - 46s 562us/sample - loss: 0.0962 - acc: 0.9696 - val_loss: 0.2828 - val_acc: 0.9513\n",
      "Epoch 94/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9677\n",
      "Epoch 00094: val_loss did not improve from 0.27010\n",
      "82114/82114 [==============================] - 46s 560us/sample - loss: 0.1020 - acc: 0.9677 - val_loss: 0.2741 - val_acc: 0.9512\n",
      "Epoch 95/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.1116 - acc: 0.9639\n",
      "Epoch 00095: val_loss did not improve from 0.27010\n",
      "82114/82114 [==============================] - 46s 562us/sample - loss: 0.1117 - acc: 0.9639 - val_loss: 0.2860 - val_acc: 0.9504\n",
      "Epoch 96/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.9695\n",
      "Epoch 00096: val_loss did not improve from 0.27010\n",
      "82114/82114 [==============================] - 46s 561us/sample - loss: 0.0975 - acc: 0.9695 - val_loss: 0.2770 - val_acc: 0.9514\n",
      "Epoch 97/100\n",
      "82048/82114 [============================>.] - ETA: 0s - loss: 0.1059 - acc: 0.9666\n",
      "Epoch 00097: val_loss did not improve from 0.27010\n",
      "82114/82114 [==============================] - 46s 562us/sample - loss: 0.1061 - acc: 0.9666 - val_loss: 0.2822 - val_acc: 0.9524\n",
      "Epoch 98/100\n",
      "82080/82114 [============================>.] - ETA: 0s - loss: 0.0899 - acc: 0.9716\n",
      "Epoch 00098: val_loss did not improve from 0.27010\n",
      "82114/82114 [==============================] - 46s 560us/sample - loss: 0.0899 - acc: 0.9716 - val_loss: 0.2832 - val_acc: 0.9530\n",
      "Epoch 99/100\n",
      "82080/82114 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9691\n",
      "Epoch 00099: val_loss did not improve from 0.27010\n",
      "82114/82114 [==============================] - 46s 560us/sample - loss: 0.0973 - acc: 0.9690 - val_loss: 0.2798 - val_acc: 0.9524\n",
      "Epoch 100/100\n",
      "82112/82114 [============================>.] - ETA: 0s - loss: 0.0900 - acc: 0.9714\n",
      "Epoch 00100: val_loss did not improve from 0.27010\n",
      "82114/82114 [==============================] - 46s 560us/sample - loss: 0.0900 - acc: 0.9714 - val_loss: 0.2845 - val_acc: 0.9524\n",
      "Wall time: 1h 18min 35s\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "%time history = model.fit(X_train, y_train, validation_split=0.1, epochs=100, verbose=1, callbacks=[checkpointer, earlystoper])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3R8fAvEiBQF"
   },
   "source": [
    "## Plot summarize history\n",
    "พล็อตดูภาพรวมตาม epoch เพื่อดูว่า accuracy และ loss มีค่าเพิ่มขึ้นหรือลดลง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "7o_Eidz7Ya_h",
    "outputId": "4d12447b-29e7-451b-e5a4-b7af3cdf186d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "d9LhSvMXYgey"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylim(0,1)\n",
    "plt.grid()\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "DaPntOu9bIWW"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylim(0, None)\n",
    "plt.grid()\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVbd1qnsiKh8"
   },
   "source": [
    "## Test model with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "oS6zzYn5T5iC"
   },
   "outputs": [],
   "source": [
    "# checkpoint_filepath = '/content/drive/MyDrive/Colab Notebooks/kuzushiji-ml-it-kmitl-2020/model/cnn_weights.h5'\n",
    "checkpoint_filepath = 'model/cnn_weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "TRBwKop4tj-L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\envs\\shine_env\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\envs\\shine_env\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\envs\\shine_env\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# โหลด weight ที่ train ไว้\n",
    "model = keras.models.load_model(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "oo_a5gYCIw8X"
   },
   "outputs": [],
   "source": [
    "# ทำนายผล\n",
    "y_pred = np.argmax(model.predict(X_val), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "lbg_5Fb3xwco"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - sparse_categorical_crossentropy: 0.02939, accuracy: 0.99466\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
    "print('Train - sparse_categorical_crossentropy: %.5f, accuracy: %.5f' % (loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "NXv4IPvTvsyN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - sparse_categorical_crossentropy: 0.22026, accuracy: 0.95867\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "loss, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "print('Validation - sparse_categorical_crossentropy: %.5f, accuracy: %.5f' % (loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "h27_5IjNgqn5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.95867\n",
      "precision: 0.95867\n",
      "recall: 0.95867\n",
      "f1_score: 0.95867\n"
     ]
    }
   ],
   "source": [
    "print('accuracy: %.5f' % accuracy_score(y_val, y_pred))\n",
    "print('precision: %.5f' % precision_score(y_val, y_pred, average=\"micro\"))\n",
    "print('recall: %.5f' % recall_score(y_val, y_pred, average=\"micro\"))\n",
    "print('f1_score: %.5f' % f1_score(y_val, y_pred, average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "AnR-Q-Ze_GAg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.90909   1.00000   0.95238        30\n",
      "           1    1.00000   1.00000   1.00000         2\n",
      "           2    0.90000   0.90000   0.90000        10\n",
      "           3    0.90000   1.00000   0.94737         9\n",
      "           4    0.69231   0.81818   0.75000        11\n",
      "           5    0.98507   1.00000   0.99248        66\n",
      "           6    1.00000   1.00000   1.00000         3\n",
      "           7    1.00000   1.00000   1.00000         2\n",
      "           8    1.00000   1.00000   1.00000         6\n",
      "           9    0.92308   1.00000   0.96000        12\n",
      "          10    1.00000   0.98551   0.99270        69\n",
      "          11    0.98361   0.95238   0.96774        63\n",
      "          12    1.00000   1.00000   1.00000        15\n",
      "          13    1.00000   0.66667   0.80000         3\n",
      "          14    0.95181   0.98750   0.96933        80\n",
      "          15    0.72727   0.88889   0.80000         9\n",
      "          16    0.95652   1.00000   0.97778        44\n",
      "          17    0.94737   1.00000   0.97297        54\n",
      "          18    1.00000   0.90909   0.95238        11\n",
      "          19    1.00000   0.98958   0.99476        96\n",
      "          20    0.96629   0.95556   0.96089        90\n",
      "          21    0.96970   1.00000   0.98462        32\n",
      "          22    1.00000   0.90000   0.94737        10\n",
      "          23    0.97674   1.00000   0.98824        42\n",
      "          24    0.99248   0.98507   0.98876       134\n",
      "          25    1.00000   0.83333   0.90909         6\n",
      "          26    0.98810   0.98810   0.98810        84\n",
      "          27    0.96667   0.96667   0.96667        30\n",
      "          28    1.00000   0.96552   0.98246        58\n",
      "          29    1.00000   1.00000   1.00000        26\n",
      "          30    0.85714   0.85714   0.85714         7\n",
      "          31    0.94118   0.94118   0.94118        17\n",
      "          32    0.95000   0.95000   0.95000        20\n",
      "          33    1.00000   0.94444   0.97143        18\n",
      "          34    0.87500   1.00000   0.93333        14\n",
      "          35    1.00000   0.92857   0.96296        28\n",
      "          36    1.00000   0.50000   0.66667         2\n",
      "          37    1.00000   0.90909   0.95238        11\n",
      "          38    1.00000   1.00000   1.00000         4\n",
      "          39    1.00000   0.96774   0.98361        31\n",
      "          40    1.00000   0.80000   0.88889         5\n",
      "          41    0.97222   1.00000   0.98592        35\n",
      "          42    0.97917   1.00000   0.98947       141\n",
      "          43    1.00000   1.00000   1.00000        13\n",
      "          44    1.00000   0.90000   0.94737        10\n",
      "          45    0.95000   0.95000   0.95000        20\n",
      "          46    1.00000   1.00000   1.00000        26\n",
      "          47    0.96667   1.00000   0.98305        58\n",
      "          48    0.98630   1.00000   0.99310        72\n",
      "          49    0.77778   1.00000   0.87500         7\n",
      "          50    1.00000   0.91667   0.95652        12\n",
      "          51    1.00000   0.96774   0.98361        31\n",
      "          52    0.98039   0.99010   0.98522       101\n",
      "          53    1.00000   1.00000   1.00000        70\n",
      "          54    0.83333   1.00000   0.90909         5\n",
      "          55    0.88889   1.00000   0.94118         8\n",
      "          56    0.95652   1.00000   0.97778        22\n",
      "          57    0.94872   1.00000   0.97368        37\n",
      "          58    1.00000   0.98413   0.99200        63\n",
      "          59    0.97590   0.98780   0.98182        82\n",
      "          60    0.88889   0.88889   0.88889         9\n",
      "          61    0.92308   1.00000   0.96000        12\n",
      "          62    0.94444   1.00000   0.97143        17\n",
      "          63    1.00000   1.00000   1.00000         5\n",
      "          64    1.00000   1.00000   1.00000        25\n",
      "          65    1.00000   1.00000   1.00000         4\n",
      "          66    1.00000   1.00000   1.00000        12\n",
      "          67    0.93333   1.00000   0.96552        42\n",
      "          68    0.88235   0.93750   0.90909        16\n",
      "          69    0.93333   1.00000   0.96552        14\n",
      "          70    1.00000   0.92308   0.96000        26\n",
      "          71    0.80000   1.00000   0.88889         4\n",
      "          72    1.00000   0.95349   0.97619        43\n",
      "          73    0.97143   1.00000   0.98551        34\n",
      "          74    0.94186   0.97590   0.95858        83\n",
      "          75    1.00000   1.00000   1.00000         5\n",
      "          76    0.90244   0.94872   0.92500        39\n",
      "          77    1.00000   1.00000   1.00000         7\n",
      "          78    0.95000   1.00000   0.97436        19\n",
      "          79    0.99206   0.99206   0.99206       126\n",
      "          80    1.00000   0.75000   0.85714         4\n",
      "          81    1.00000   1.00000   1.00000         6\n",
      "          82    0.98750   0.98750   0.98750        80\n",
      "          83    1.00000   1.00000   1.00000        13\n",
      "          84    0.60000   1.00000   0.75000         3\n",
      "          85    0.83333   0.83333   0.83333         6\n",
      "          86    1.00000   1.00000   1.00000        21\n",
      "          87    1.00000   1.00000   1.00000        44\n",
      "          88    1.00000   1.00000   1.00000         7\n",
      "          89    1.00000   1.00000   1.00000        15\n",
      "          90    1.00000   1.00000   1.00000        16\n",
      "          91    0.90000   1.00000   0.94737         9\n",
      "          92    1.00000   0.90000   0.94737        10\n",
      "          93    1.00000   1.00000   1.00000         5\n",
      "          94    1.00000   0.91667   0.95652        12\n",
      "          95    1.00000   0.75000   0.85714         4\n",
      "          96    0.98148   1.00000   0.99065        53\n",
      "          97    1.00000   1.00000   1.00000        11\n",
      "          98    1.00000   1.00000   1.00000         7\n",
      "          99    1.00000   1.00000   1.00000        21\n",
      "         100    1.00000   1.00000   1.00000        16\n",
      "         101    1.00000   1.00000   1.00000         2\n",
      "         102    0.99301   1.00000   0.99649       142\n",
      "         103    1.00000   1.00000   1.00000         4\n",
      "         104    1.00000   1.00000   1.00000        23\n",
      "         105    0.95745   0.97826   0.96774        46\n",
      "         106    1.00000   1.00000   1.00000         2\n",
      "         107    1.00000   0.91667   0.95652        12\n",
      "         108    0.90476   0.90476   0.90476        21\n",
      "         109    1.00000   1.00000   1.00000        15\n",
      "         110    1.00000   1.00000   1.00000         2\n",
      "         111    0.88889   1.00000   0.94118         8\n",
      "         112    1.00000   1.00000   1.00000         7\n",
      "         113    0.95918   1.00000   0.97917        47\n",
      "         114    0.98148   1.00000   0.99065        53\n",
      "         115    0.95455   1.00000   0.97674        42\n",
      "         116    1.00000   1.00000   1.00000        13\n",
      "         117    1.00000   1.00000   1.00000         2\n",
      "         118    0.90476   1.00000   0.95000        19\n",
      "         119    1.00000   1.00000   1.00000        21\n",
      "         120    1.00000   1.00000   1.00000         4\n",
      "         121    1.00000   1.00000   1.00000         2\n",
      "         122    1.00000   0.96000   0.97959        25\n",
      "         123    0.97500   1.00000   0.98734        39\n",
      "         124    1.00000   1.00000   1.00000        17\n",
      "         125    1.00000   0.96000   0.97959        25\n",
      "         126    1.00000   1.00000   1.00000         3\n",
      "         127    1.00000   0.96000   0.97959        25\n",
      "         128    1.00000   0.96774   0.98361        31\n",
      "         129    1.00000   1.00000   1.00000         5\n",
      "         130    1.00000   1.00000   1.00000        30\n",
      "         131    1.00000   1.00000   1.00000         8\n",
      "         132    0.93103   0.96429   0.94737        28\n",
      "         133    0.83333   0.83333   0.83333         6\n",
      "         134    0.87500   1.00000   0.93333         7\n",
      "         135    1.00000   0.80000   0.88889         5\n",
      "         136    1.00000   0.83333   0.90909        12\n",
      "         137    1.00000   1.00000   1.00000        10\n",
      "         138    1.00000   1.00000   1.00000         5\n",
      "         139    0.86667   0.92857   0.89655        14\n",
      "         140    1.00000   0.92308   0.96000        13\n",
      "         141    1.00000   1.00000   1.00000         3\n",
      "         142    1.00000   0.95238   0.97561        21\n",
      "         143    0.97368   1.00000   0.98667        74\n",
      "         144    1.00000   1.00000   1.00000         3\n",
      "         145    1.00000   1.00000   1.00000        11\n",
      "         146    1.00000   1.00000   1.00000        32\n",
      "         147    1.00000   0.95455   0.97674        22\n",
      "         148    1.00000   1.00000   1.00000        11\n",
      "         149    1.00000   1.00000   1.00000        11\n",
      "         150    1.00000   0.85714   0.92308         7\n",
      "         151    0.66667   1.00000   0.80000         2\n",
      "         152    1.00000   1.00000   1.00000         2\n",
      "         153    1.00000   1.00000   1.00000        30\n",
      "         154    0.75000   0.60000   0.66667         5\n",
      "         155    1.00000   1.00000   1.00000        13\n",
      "         156    0.88889   0.88889   0.88889         9\n",
      "         157    1.00000   1.00000   1.00000         5\n",
      "         158    1.00000   0.88889   0.94118         9\n",
      "         159    0.75000   0.90000   0.81818        10\n",
      "         160    0.94643   1.00000   0.97248        53\n",
      "         161    0.94915   0.96552   0.95726        58\n",
      "         162    0.90000   1.00000   0.94737         9\n",
      "         163    1.00000   1.00000   1.00000         2\n",
      "         164    0.98000   0.98000   0.98000        50\n",
      "         165    1.00000   0.83333   0.90909         6\n",
      "         166    0.96875   1.00000   0.98413        31\n",
      "         167    0.90909   1.00000   0.95238        10\n",
      "         168    0.96154   1.00000   0.98039        50\n",
      "         169    1.00000   1.00000   1.00000        23\n",
      "         170    1.00000   0.90000   0.94737        10\n",
      "         171    1.00000   1.00000   1.00000        18\n",
      "         172    1.00000   1.00000   1.00000        12\n",
      "         173    1.00000   1.00000   1.00000        25\n",
      "         174    1.00000   1.00000   1.00000         9\n",
      "         175    1.00000   1.00000   1.00000        19\n",
      "         176    0.94118   0.97959   0.96000        49\n",
      "         177    0.90000   1.00000   0.94737         9\n",
      "         178    1.00000   1.00000   1.00000         3\n",
      "         179    1.00000   0.66667   0.80000         3\n",
      "         180    0.94595   1.00000   0.97222        35\n",
      "         181    0.84615   1.00000   0.91667        11\n",
      "         182    0.96875   1.00000   0.98413        31\n",
      "         183    0.94444   1.00000   0.97143        17\n",
      "         184    1.00000   0.97619   0.98795        42\n",
      "         185    0.95918   1.00000   0.97917        47\n",
      "         186    1.00000   1.00000   1.00000         6\n",
      "         187    0.94118   0.88889   0.91429        18\n",
      "         188    1.00000   1.00000   1.00000         5\n",
      "         189    1.00000   1.00000   1.00000         3\n",
      "         190    1.00000   0.90000   0.94737        10\n",
      "         191    1.00000   0.96667   0.98305        30\n",
      "         192    1.00000   1.00000   1.00000         4\n",
      "         193    1.00000   0.87500   0.93333         8\n",
      "         194    0.75000   1.00000   0.85714         6\n",
      "         195    1.00000   0.66667   0.80000         6\n",
      "         196    0.66667   1.00000   0.80000         2\n",
      "         197    1.00000   1.00000   1.00000        31\n",
      "         198    1.00000   0.50000   0.66667         2\n",
      "         199    1.00000   1.00000   1.00000         2\n",
      "         200    0.90909   1.00000   0.95238        10\n",
      "         201    0.75000   0.75000   0.75000         4\n",
      "         202    1.00000   1.00000   1.00000         8\n",
      "         203    1.00000   1.00000   1.00000         6\n",
      "         204    1.00000   0.66667   0.80000         6\n",
      "         205    0.70000   1.00000   0.82353         7\n",
      "         206    1.00000   1.00000   1.00000        34\n",
      "         207    1.00000   1.00000   1.00000         4\n",
      "         208    1.00000   1.00000   1.00000        28\n",
      "         209    1.00000   1.00000   1.00000        22\n",
      "         210    1.00000   1.00000   1.00000        85\n",
      "         211    0.75000   1.00000   0.85714         3\n",
      "         212    0.88889   1.00000   0.94118         8\n",
      "         213    1.00000   0.90909   0.95238        11\n",
      "         214    1.00000   1.00000   1.00000         4\n",
      "         215    1.00000   0.84615   0.91667        13\n",
      "         216    1.00000   0.85714   0.92308         7\n",
      "         217    0.50000   0.50000   0.50000         2\n",
      "         218    1.00000   0.83333   0.90909         6\n",
      "         219    0.75000   0.75000   0.75000         4\n",
      "         220    1.00000   0.99138   0.99567       116\n",
      "         221    0.93750   0.88235   0.90909        17\n",
      "         222    0.92308   0.92308   0.92308        13\n",
      "         223    1.00000   0.85714   0.92308         7\n",
      "         224    1.00000   0.88889   0.94118         9\n",
      "         225    1.00000   0.83333   0.90909         6\n",
      "         226    1.00000   1.00000   1.00000        19\n",
      "         227    1.00000   1.00000   1.00000         7\n",
      "         228    1.00000   1.00000   1.00000        16\n",
      "         229    1.00000   0.75000   0.85714        12\n",
      "         230    0.88889   1.00000   0.94118         8\n",
      "         231    0.87500   1.00000   0.93333         7\n",
      "         232    1.00000   1.00000   1.00000         3\n",
      "         233    1.00000   0.95238   0.97561        42\n",
      "         234    0.95455   1.00000   0.97674        21\n",
      "         235    0.96875   0.93939   0.95385        33\n",
      "         236    1.00000   1.00000   1.00000         7\n",
      "         237    1.00000   1.00000   1.00000         9\n",
      "         238    0.98000   0.96078   0.97030        51\n",
      "         239    1.00000   1.00000   1.00000         6\n",
      "         240    1.00000   0.85714   0.92308         7\n",
      "         241    0.95238   0.97561   0.96386        41\n",
      "         242    0.96154   1.00000   0.98039        25\n",
      "         243    1.00000   0.66667   0.80000         3\n",
      "         244    0.90000   1.00000   0.94737         9\n",
      "         245    0.85714   1.00000   0.92308         6\n",
      "         246    1.00000   1.00000   1.00000         4\n",
      "         247    1.00000   0.66667   0.80000         3\n",
      "         248    0.50000   1.00000   0.66667         2\n",
      "         249    1.00000   1.00000   1.00000         4\n",
      "         250    1.00000   0.88889   0.94118         9\n",
      "         251    1.00000   0.33333   0.50000         3\n",
      "         252    0.75000   1.00000   0.85714         3\n",
      "         253    0.95238   1.00000   0.97561        20\n",
      "         254    0.90909   1.00000   0.95238        10\n",
      "         255    1.00000   1.00000   1.00000        12\n",
      "         256    1.00000   1.00000   1.00000        13\n",
      "         257    0.90000   1.00000   0.94737         9\n",
      "         258    0.90000   0.90000   0.90000        10\n",
      "         259    0.83333   1.00000   0.90909         5\n",
      "         260    1.00000   1.00000   1.00000         2\n",
      "         261    1.00000   0.33333   0.50000         3\n",
      "         262    1.00000   1.00000   1.00000         4\n",
      "         263    0.83333   1.00000   0.90909         5\n",
      "         264    1.00000   1.00000   1.00000         2\n",
      "         265    1.00000   1.00000   1.00000         4\n",
      "         266    1.00000   1.00000   1.00000         6\n",
      "         267    1.00000   1.00000   1.00000         2\n",
      "         268    1.00000   1.00000   1.00000         5\n",
      "         269    1.00000   1.00000   1.00000        21\n",
      "         270    1.00000   0.80000   0.88889         5\n",
      "         271    1.00000   1.00000   1.00000         8\n",
      "         272    0.91667   1.00000   0.95652        22\n",
      "         273    1.00000   1.00000   1.00000        18\n",
      "         274    1.00000   1.00000   1.00000         3\n",
      "         275    1.00000   1.00000   1.00000        14\n",
      "         276    1.00000   1.00000   1.00000         6\n",
      "         277    1.00000   1.00000   1.00000         5\n",
      "         278    0.87500   1.00000   0.93333        14\n",
      "         279    0.87500   1.00000   0.93333         7\n",
      "         280    1.00000   1.00000   1.00000        31\n",
      "         281    1.00000   0.50000   0.66667         2\n",
      "         282    0.92500   1.00000   0.96104        37\n",
      "         283    0.91667   1.00000   0.95652        11\n",
      "         284    1.00000   1.00000   1.00000        19\n",
      "         285    1.00000   1.00000   1.00000        15\n",
      "         286    1.00000   1.00000   1.00000         3\n",
      "         287    1.00000   1.00000   1.00000        44\n",
      "         288    1.00000   1.00000   1.00000         3\n",
      "         289    1.00000   1.00000   1.00000         9\n",
      "         290    0.95238   1.00000   0.97561        20\n",
      "         291    1.00000   1.00000   1.00000         3\n",
      "         292    0.00000   0.00000   0.00000         2\n",
      "         293    1.00000   1.00000   1.00000         3\n",
      "         294    0.80000   0.80000   0.80000         5\n",
      "         295    1.00000   0.66667   0.80000         3\n",
      "         296    0.92857   1.00000   0.96296        13\n",
      "         297    1.00000   1.00000   1.00000         5\n",
      "         298    1.00000   1.00000   1.00000         5\n",
      "         299    0.92857   1.00000   0.96296        13\n",
      "         300    1.00000   0.50000   0.66667         4\n",
      "         301    1.00000   1.00000   1.00000        11\n",
      "         302    0.82759   1.00000   0.90566        24\n",
      "         303    1.00000   1.00000   1.00000         2\n",
      "         304    1.00000   1.00000   1.00000         2\n",
      "         305    1.00000   0.77778   0.87500         9\n",
      "         306    0.89474   1.00000   0.94444        17\n",
      "         307    1.00000   0.66667   0.80000         3\n",
      "         308    1.00000   1.00000   1.00000         4\n",
      "         309    0.83333   1.00000   0.90909        10\n",
      "         310    1.00000   1.00000   1.00000         4\n",
      "         311    1.00000   1.00000   1.00000         7\n",
      "         312    1.00000   0.83333   0.90909         6\n",
      "         313    1.00000   1.00000   1.00000         2\n",
      "         314    1.00000   1.00000   1.00000         5\n",
      "         315    0.85714   0.85714   0.85714         7\n",
      "         316    1.00000   0.66667   0.80000         3\n",
      "         317    0.85714   1.00000   0.92308         6\n",
      "         318    1.00000   1.00000   1.00000         8\n",
      "         319    0.86667   0.81250   0.83871        16\n",
      "         320    1.00000   1.00000   1.00000         2\n",
      "         321    0.95238   0.95238   0.95238        21\n",
      "         322    0.85714   1.00000   0.92308         6\n",
      "         323    1.00000   1.00000   1.00000         3\n",
      "         324    1.00000   0.90000   0.94737        10\n",
      "         325    1.00000   1.00000   1.00000         5\n",
      "         326    0.97368   0.92500   0.94872        40\n",
      "         327    0.97500   1.00000   0.98734        39\n",
      "         328    1.00000   0.94737   0.97297        19\n",
      "         329    0.80000   1.00000   0.88889         4\n",
      "         330    0.97059   1.00000   0.98507        33\n",
      "         331    1.00000   0.75000   0.85714         4\n",
      "         332    1.00000   0.94444   0.97143        18\n",
      "         333    0.75000   1.00000   0.85714         6\n",
      "         334    1.00000   0.85714   0.92308         7\n",
      "         335    1.00000   0.85714   0.92308         7\n",
      "         336    1.00000   0.66667   0.80000         3\n",
      "         337    1.00000   0.50000   0.66667         2\n",
      "         338    1.00000   0.80000   0.88889         5\n",
      "         339    1.00000   0.94444   0.97143        18\n",
      "         340    1.00000   0.92593   0.96154        27\n",
      "         341    0.97143   0.97143   0.97143        35\n",
      "         342    1.00000   0.85714   0.92308         7\n",
      "         343    1.00000   1.00000   1.00000        11\n",
      "         344    1.00000   1.00000   1.00000         9\n",
      "         345    0.66667   1.00000   0.80000         2\n",
      "         346    0.85714   1.00000   0.92308         6\n",
      "         347    1.00000   1.00000   1.00000         2\n",
      "         348    1.00000   1.00000   1.00000         4\n",
      "         349    0.75000   1.00000   0.85714         6\n",
      "         350    0.66667   0.66667   0.66667         3\n",
      "         351    0.87500   1.00000   0.93333        14\n",
      "         352    0.93750   1.00000   0.96774        15\n",
      "         353    1.00000   0.97059   0.98507        34\n",
      "         354    1.00000   0.50000   0.66667         2\n",
      "         355    1.00000   1.00000   1.00000         5\n",
      "         356    1.00000   0.92308   0.96000        13\n",
      "         357    1.00000   0.90000   0.94737        10\n",
      "         358    1.00000   1.00000   1.00000         9\n",
      "         359    0.96774   1.00000   0.98361        30\n",
      "         360    1.00000   1.00000   1.00000         4\n",
      "         361    1.00000   1.00000   1.00000        10\n",
      "         362    1.00000   1.00000   1.00000         7\n",
      "         363    1.00000   0.97500   0.98734        40\n",
      "         364    1.00000   1.00000   1.00000         6\n",
      "         365    0.88889   0.88889   0.88889         9\n",
      "         366    0.66667   0.66667   0.66667         3\n",
      "         367    1.00000   1.00000   1.00000        18\n",
      "         368    1.00000   0.84615   0.91667        13\n",
      "         369    1.00000   1.00000   1.00000        14\n",
      "         370    1.00000   1.00000   1.00000        13\n",
      "         371    0.50000   0.50000   0.50000         2\n",
      "         372    0.97619   0.97619   0.97619        42\n",
      "         373    1.00000   0.33333   0.50000         3\n",
      "         374    1.00000   1.00000   1.00000        11\n",
      "         375    0.80000   0.88889   0.84211         9\n",
      "         376    1.00000   1.00000   1.00000        10\n",
      "         377    0.85714   1.00000   0.92308         6\n",
      "         378    1.00000   0.95238   0.97561        21\n",
      "         379    1.00000   1.00000   1.00000         7\n",
      "         380    1.00000   1.00000   1.00000         2\n",
      "         381    1.00000   1.00000   1.00000         8\n",
      "         382    0.75000   1.00000   0.85714         3\n",
      "         383    1.00000   1.00000   1.00000        10\n",
      "         384    1.00000   1.00000   1.00000         2\n",
      "         385    1.00000   1.00000   1.00000         2\n",
      "         386    1.00000   0.83333   0.90909         6\n",
      "         387    1.00000   0.66667   0.80000         3\n",
      "         388    1.00000   1.00000   1.00000         2\n",
      "         389    0.90000   1.00000   0.94737         9\n",
      "         390    0.75000   1.00000   0.85714         6\n",
      "         391    1.00000   1.00000   1.00000         7\n",
      "         392    1.00000   0.75000   0.85714         4\n",
      "         393    0.85714   1.00000   0.92308         6\n",
      "         394    1.00000   1.00000   1.00000         2\n",
      "         395    1.00000   1.00000   1.00000         8\n",
      "         396    1.00000   1.00000   1.00000        11\n",
      "         397    1.00000   0.81818   0.90000        11\n",
      "         398    0.87500   0.87500   0.87500         8\n",
      "         399    1.00000   0.66667   0.80000         3\n",
      "         400    1.00000   1.00000   1.00000        15\n",
      "         401    1.00000   1.00000   1.00000         3\n",
      "         402    1.00000   1.00000   1.00000         6\n",
      "         403    0.89474   1.00000   0.94444        17\n",
      "         404    1.00000   1.00000   1.00000         2\n",
      "         405    0.57143   1.00000   0.72727         4\n",
      "         406    1.00000   1.00000   1.00000         5\n",
      "         407    1.00000   1.00000   1.00000         5\n",
      "         408    0.80000   0.80000   0.80000         5\n",
      "         409    0.97143   1.00000   0.98551        34\n",
      "         410    1.00000   1.00000   1.00000         9\n",
      "         411    1.00000   1.00000   1.00000         5\n",
      "         412    0.66667   0.66667   0.66667         3\n",
      "         413    0.90909   1.00000   0.95238        10\n",
      "         414    1.00000   0.75000   0.85714         4\n",
      "         415    1.00000   1.00000   1.00000         2\n",
      "         416    1.00000   1.00000   1.00000        16\n",
      "         417    1.00000   1.00000   1.00000         2\n",
      "         418    0.75000   0.75000   0.75000         4\n",
      "         419    0.90909   1.00000   0.95238        10\n",
      "         420    0.93103   0.93103   0.93103        29\n",
      "         421    1.00000   1.00000   1.00000         3\n",
      "         422    1.00000   1.00000   1.00000         5\n",
      "         423    1.00000   1.00000   1.00000         9\n",
      "         424    0.66667   1.00000   0.80000         4\n",
      "         425    1.00000   1.00000   1.00000         7\n",
      "         426    0.83333   1.00000   0.90909         5\n",
      "         427    0.80000   1.00000   0.88889         4\n",
      "         428    0.94444   0.94444   0.94444        18\n",
      "         429    0.00000   0.00000   0.00000         2\n",
      "         430    1.00000   1.00000   1.00000        11\n",
      "         431    1.00000   0.89474   0.94444        19\n",
      "         432    0.85714   1.00000   0.92308         6\n",
      "         433    1.00000   1.00000   1.00000         5\n",
      "         434    0.66667   1.00000   0.80000         2\n",
      "         435    1.00000   1.00000   1.00000         2\n",
      "         436    1.00000   1.00000   1.00000         5\n",
      "         437    1.00000   0.66667   0.80000         6\n",
      "         438    0.83333   1.00000   0.90909         5\n",
      "         439    1.00000   0.50000   0.66667         2\n",
      "         440    0.85714   0.85714   0.85714         7\n",
      "         441    0.71429   1.00000   0.83333         5\n",
      "         442    1.00000   1.00000   1.00000        10\n",
      "         443    1.00000   1.00000   1.00000         3\n",
      "         444    1.00000   1.00000   1.00000         5\n",
      "         445    1.00000   0.50000   0.66667         2\n",
      "         446    0.90000   1.00000   0.94737         9\n",
      "         447    0.71429   0.83333   0.76923         6\n",
      "         448    1.00000   1.00000   1.00000        13\n",
      "         449    1.00000   1.00000   1.00000         9\n",
      "         450    1.00000   0.66667   0.80000         6\n",
      "         451    1.00000   1.00000   1.00000         4\n",
      "         452    1.00000   1.00000   1.00000         3\n",
      "         453    1.00000   1.00000   1.00000         4\n",
      "         454    0.80000   1.00000   0.88889         4\n",
      "         455    0.85714   1.00000   0.92308         6\n",
      "         456    0.94118   1.00000   0.96970        16\n",
      "         457    1.00000   1.00000   1.00000         2\n",
      "         458    1.00000   1.00000   1.00000        10\n",
      "         459    0.70000   1.00000   0.82353         7\n",
      "         460    1.00000   1.00000   1.00000         5\n",
      "         461    1.00000   1.00000   1.00000         2\n",
      "         462    1.00000   1.00000   1.00000         3\n",
      "         463    0.82609   0.90476   0.86364        21\n",
      "         464    1.00000   1.00000   1.00000        16\n",
      "         465    1.00000   0.90000   0.94737        10\n",
      "         466    0.91667   1.00000   0.95652        11\n",
      "         467    1.00000   1.00000   1.00000         4\n",
      "         468    0.81818   0.81818   0.81818        11\n",
      "         469    1.00000   1.00000   1.00000         8\n",
      "         470    1.00000   1.00000   1.00000         2\n",
      "         471    1.00000   1.00000   1.00000         9\n",
      "         472    1.00000   0.90000   0.94737        10\n",
      "         473    1.00000   1.00000   1.00000         3\n",
      "         474    1.00000   1.00000   1.00000        12\n",
      "         475    0.66667   1.00000   0.80000         2\n",
      "         476    1.00000   1.00000   1.00000         5\n",
      "         477    1.00000   0.85714   0.92308         7\n",
      "         478    0.50000   0.50000   0.50000         2\n",
      "         479    1.00000   1.00000   1.00000         3\n",
      "         480    1.00000   0.83333   0.90909         6\n",
      "         481    0.50000   0.50000   0.50000         2\n",
      "         482    1.00000   1.00000   1.00000         9\n",
      "         483    1.00000   0.88000   0.93617        25\n",
      "         484    1.00000   1.00000   1.00000         4\n",
      "         485    1.00000   0.90909   0.95238        11\n",
      "         486    0.90909   1.00000   0.95238        10\n",
      "         487    1.00000   1.00000   1.00000        10\n",
      "         488    1.00000   1.00000   1.00000         3\n",
      "         489    1.00000   0.60000   0.75000         5\n",
      "         490    1.00000   1.00000   1.00000         2\n",
      "         491    1.00000   1.00000   1.00000        10\n",
      "         492    0.92308   0.92308   0.92308        13\n",
      "         493    1.00000   1.00000   1.00000         3\n",
      "         494    1.00000   1.00000   1.00000         2\n",
      "         495    1.00000   1.00000   1.00000         6\n",
      "         496    1.00000   1.00000   1.00000         6\n",
      "         497    1.00000   1.00000   1.00000         6\n",
      "         498    0.90000   0.90000   0.90000        10\n",
      "         499    1.00000   1.00000   1.00000         3\n",
      "         500    1.00000   1.00000   1.00000         6\n",
      "         501    1.00000   1.00000   1.00000         3\n",
      "         502    1.00000   1.00000   1.00000         6\n",
      "         503    1.00000   0.83333   0.90909         6\n",
      "         504    1.00000   1.00000   1.00000         2\n",
      "         505    0.66667   1.00000   0.80000         4\n",
      "         506    1.00000   1.00000   1.00000         6\n",
      "         507    0.50000   0.33333   0.40000         3\n",
      "         508    1.00000   0.57143   0.72727         7\n",
      "         509    1.00000   1.00000   1.00000         3\n",
      "         510    1.00000   0.66667   0.80000         3\n",
      "         511    0.87500   1.00000   0.93333         7\n",
      "         512    1.00000   1.00000   1.00000         2\n",
      "         513    1.00000   1.00000   1.00000         3\n",
      "         514    1.00000   1.00000   1.00000        11\n",
      "         515    1.00000   1.00000   1.00000         2\n",
      "         516    1.00000   1.00000   1.00000        11\n",
      "         517    1.00000   1.00000   1.00000         6\n",
      "         518    1.00000   0.81818   0.90000        11\n",
      "         519    1.00000   1.00000   1.00000         2\n",
      "         520    0.87500   0.93333   0.90323        15\n",
      "         521    1.00000   0.90909   0.95238        11\n",
      "         522    0.80000   1.00000   0.88889         4\n",
      "         523    0.71429   0.71429   0.71429         7\n",
      "         524    1.00000   1.00000   1.00000         2\n",
      "         525    1.00000   0.50000   0.66667         2\n",
      "         526    0.81818   1.00000   0.90000         9\n",
      "         527    0.85714   1.00000   0.92308         6\n",
      "         528    1.00000   0.85714   0.92308         7\n",
      "         529    1.00000   0.93750   0.96774        16\n",
      "         530    1.00000   1.00000   1.00000         2\n",
      "         531    1.00000   1.00000   1.00000         5\n",
      "         532    0.75000   1.00000   0.85714         3\n",
      "         533    1.00000   1.00000   1.00000        15\n",
      "         534    1.00000   1.00000   1.00000         5\n",
      "         535    1.00000   1.00000   1.00000         2\n",
      "         536    1.00000   1.00000   1.00000        10\n",
      "         537    0.75000   0.60000   0.66667         5\n",
      "         538    1.00000   1.00000   1.00000         2\n",
      "         539    0.90909   0.90909   0.90909        11\n",
      "         540    0.83333   1.00000   0.90909         5\n",
      "         541    1.00000   0.94118   0.96970        17\n",
      "         542    1.00000   0.66667   0.80000         3\n",
      "         543    1.00000   1.00000   1.00000         6\n",
      "         544    1.00000   0.50000   0.66667         2\n",
      "         545    1.00000   0.75000   0.85714         4\n",
      "         546    0.66667   0.66667   0.66667         3\n",
      "         547    1.00000   0.66667   0.80000         3\n",
      "         548    1.00000   1.00000   1.00000         2\n",
      "         549    0.92857   1.00000   0.96296        13\n",
      "         550    0.75000   1.00000   0.85714         6\n",
      "         551    1.00000   1.00000   1.00000         8\n",
      "         552    0.92857   0.92857   0.92857        14\n",
      "         553    0.75000   0.85714   0.80000         7\n",
      "         554    1.00000   1.00000   1.00000         6\n",
      "         555    1.00000   1.00000   1.00000         3\n",
      "         556    1.00000   0.80000   0.88889        15\n",
      "         557    0.88889   1.00000   0.94118         8\n",
      "         558    1.00000   1.00000   1.00000         6\n",
      "         559    1.00000   1.00000   1.00000         2\n",
      "         560    1.00000   1.00000   1.00000        17\n",
      "         561    1.00000   1.00000   1.00000         2\n",
      "         562    0.50000   1.00000   0.66667         2\n",
      "         563    1.00000   0.80000   0.88889         5\n",
      "         564    1.00000   1.00000   1.00000         4\n",
      "         565    1.00000   1.00000   1.00000        12\n",
      "         566    1.00000   1.00000   1.00000         5\n",
      "         567    1.00000   0.83333   0.90909         6\n",
      "         568    0.75000   0.75000   0.75000         4\n",
      "         569    1.00000   1.00000   1.00000         7\n",
      "         570    1.00000   1.00000   1.00000         2\n",
      "         571    0.83333   1.00000   0.90909         5\n",
      "         572    1.00000   1.00000   1.00000        12\n",
      "         573    1.00000   1.00000   1.00000         2\n",
      "         574    1.00000   1.00000   1.00000         3\n",
      "         575    1.00000   1.00000   1.00000         2\n",
      "         576    1.00000   0.71429   0.83333         7\n",
      "         577    0.66667   1.00000   0.80000         2\n",
      "         578    0.88889   1.00000   0.94118         8\n",
      "         579    1.00000   0.66667   0.80000         3\n",
      "         580    1.00000   1.00000   1.00000         3\n",
      "         581    1.00000   1.00000   1.00000        16\n",
      "         582    0.90000   1.00000   0.94737         9\n",
      "         583    1.00000   1.00000   1.00000         3\n",
      "         584    1.00000   1.00000   1.00000        21\n",
      "         585    0.50000   1.00000   0.66667         2\n",
      "         586    1.00000   1.00000   1.00000         3\n",
      "         587    1.00000   1.00000   1.00000         3\n",
      "         588    0.90909   1.00000   0.95238        10\n",
      "         589    0.66667   1.00000   0.80000         2\n",
      "         590    1.00000   1.00000   1.00000        14\n",
      "         591    1.00000   0.75000   0.85714         4\n",
      "         592    0.75000   1.00000   0.85714         3\n",
      "         593    1.00000   1.00000   1.00000        16\n",
      "         594    0.83333   1.00000   0.90909         5\n",
      "         595    1.00000   0.88889   0.94118         9\n",
      "         596    1.00000   0.66667   0.80000         3\n",
      "         597    1.00000   0.85714   0.92308         7\n",
      "         598    0.66667   1.00000   0.80000         2\n",
      "         599    1.00000   1.00000   1.00000         3\n",
      "         600    1.00000   0.50000   0.66667         2\n",
      "         601    1.00000   1.00000   1.00000         5\n",
      "         602    1.00000   1.00000   1.00000        20\n",
      "         603    1.00000   1.00000   1.00000         2\n",
      "         604    1.00000   1.00000   1.00000         8\n",
      "         605    1.00000   1.00000   1.00000         2\n",
      "         606    0.85714   0.75000   0.80000         8\n",
      "         607    1.00000   1.00000   1.00000         4\n",
      "         608    1.00000   0.75000   0.85714         4\n",
      "         609    0.83333   1.00000   0.90909         5\n",
      "         610    0.80000   1.00000   0.88889         4\n",
      "         611    1.00000   0.66667   0.80000         3\n",
      "         612    0.80000   1.00000   0.88889         4\n",
      "         613    1.00000   1.00000   1.00000         2\n",
      "         614    1.00000   1.00000   1.00000         9\n",
      "         615    1.00000   0.66667   0.80000         3\n",
      "         616    0.83333   0.83333   0.83333         6\n",
      "         617    1.00000   1.00000   1.00000         4\n",
      "         618    0.90909   1.00000   0.95238        10\n",
      "         619    1.00000   1.00000   1.00000         2\n",
      "         620    1.00000   1.00000   1.00000         3\n",
      "         621    1.00000   1.00000   1.00000         5\n",
      "         622    1.00000   0.83333   0.90909         6\n",
      "         623    1.00000   1.00000   1.00000         2\n",
      "         624    1.00000   0.88889   0.94118         9\n",
      "         625    1.00000   0.50000   0.66667         2\n",
      "         626    1.00000   0.50000   0.66667         2\n",
      "         627    1.00000   1.00000   1.00000         6\n",
      "         628    1.00000   0.57143   0.72727         7\n",
      "         629    1.00000   1.00000   1.00000         5\n",
      "         630    1.00000   1.00000   1.00000         2\n",
      "         631    1.00000   1.00000   1.00000         3\n",
      "         632    1.00000   1.00000   1.00000         4\n",
      "         633    1.00000   1.00000   1.00000         8\n",
      "         634    1.00000   0.87500   0.93333        16\n",
      "         635    1.00000   1.00000   1.00000         4\n",
      "         636    0.94118   0.80000   0.86486        20\n",
      "         637    0.80000   0.80000   0.80000         5\n",
      "         638    1.00000   1.00000   1.00000         2\n",
      "         639    0.71429   1.00000   0.83333         5\n",
      "         640    1.00000   0.66667   0.80000         3\n",
      "         641    0.75000   1.00000   0.85714         3\n",
      "         642    1.00000   1.00000   1.00000         2\n",
      "         643    1.00000   1.00000   1.00000         4\n",
      "         644    1.00000   0.33333   0.50000         3\n",
      "         645    1.00000   0.50000   0.66667         2\n",
      "         646    0.66667   1.00000   0.80000         2\n",
      "         647    1.00000   1.00000   1.00000         3\n",
      "         648    1.00000   1.00000   1.00000        11\n",
      "         649    1.00000   1.00000   1.00000         3\n",
      "         650    1.00000   1.00000   1.00000         4\n",
      "         651    1.00000   1.00000   1.00000         5\n",
      "         652    1.00000   1.00000   1.00000         3\n",
      "         653    1.00000   1.00000   1.00000         8\n",
      "         654    0.87500   0.77778   0.82353         9\n",
      "         655    1.00000   1.00000   1.00000         4\n",
      "         656    1.00000   0.66667   0.80000         3\n",
      "         657    1.00000   1.00000   1.00000         5\n",
      "         658    1.00000   1.00000   1.00000         3\n",
      "         659    1.00000   1.00000   1.00000         2\n",
      "         660    1.00000   1.00000   1.00000         2\n",
      "         661    1.00000   1.00000   1.00000         3\n",
      "         662    1.00000   1.00000   1.00000         2\n",
      "         663    0.87500   0.73684   0.80000        19\n",
      "         664    1.00000   1.00000   1.00000         4\n",
      "         665    1.00000   1.00000   1.00000         2\n",
      "         666    1.00000   1.00000   1.00000         2\n",
      "         667    1.00000   1.00000   1.00000         6\n",
      "         668    1.00000   1.00000   1.00000         2\n",
      "         669    1.00000   0.80000   0.88889         5\n",
      "         670    0.66667   1.00000   0.80000         2\n",
      "         671    1.00000   1.00000   1.00000         2\n",
      "         672    0.80000   0.80000   0.80000         5\n",
      "         673    1.00000   0.50000   0.66667         2\n",
      "         674    1.00000   1.00000   1.00000         2\n",
      "         675    0.75000   1.00000   0.85714         3\n",
      "         676    1.00000   0.75000   0.85714         4\n",
      "         677    1.00000   1.00000   1.00000         2\n",
      "         678    1.00000   0.66667   0.80000         3\n",
      "         679    1.00000   1.00000   1.00000         3\n",
      "         680    1.00000   1.00000   1.00000         9\n",
      "         681    0.80000   1.00000   0.88889         4\n",
      "         682    1.00000   1.00000   1.00000         2\n",
      "         683    1.00000   1.00000   1.00000         3\n",
      "         684    1.00000   1.00000   1.00000         5\n",
      "         685    1.00000   1.00000   1.00000         4\n",
      "         686    1.00000   1.00000   1.00000         4\n",
      "         687    0.75000   1.00000   0.85714         3\n",
      "         688    1.00000   1.00000   1.00000         3\n",
      "         689    0.66667   1.00000   0.80000         2\n",
      "         690    1.00000   0.80000   0.88889         5\n",
      "         691    0.66667   1.00000   0.80000         6\n",
      "         692    1.00000   1.00000   1.00000         5\n",
      "         693    1.00000   1.00000   1.00000         2\n",
      "         694    1.00000   0.75000   0.85714         4\n",
      "         695    1.00000   1.00000   1.00000         4\n",
      "         696    1.00000   1.00000   1.00000         5\n",
      "         697    1.00000   1.00000   1.00000         8\n",
      "         698    0.50000   0.50000   0.50000         2\n",
      "         699    1.00000   0.80000   0.88889         5\n",
      "         700    0.75000   0.75000   0.75000         4\n",
      "         701    1.00000   0.85714   0.92308         7\n",
      "         702    0.83333   1.00000   0.90909         5\n",
      "         703    1.00000   1.00000   1.00000        11\n",
      "         704    1.00000   1.00000   1.00000         2\n",
      "         705    1.00000   0.75000   0.85714         4\n",
      "         706    1.00000   1.00000   1.00000         3\n",
      "         707    0.66667   1.00000   0.80000         2\n",
      "         708    1.00000   1.00000   1.00000         7\n",
      "         709    0.33333   0.50000   0.40000         2\n",
      "         710    1.00000   0.75000   0.85714         4\n",
      "         711    1.00000   1.00000   1.00000         2\n",
      "         712    1.00000   1.00000   1.00000        14\n",
      "         713    1.00000   1.00000   1.00000         2\n",
      "         714    1.00000   1.00000   1.00000        11\n",
      "         715    0.87500   1.00000   0.93333         7\n",
      "         716    1.00000   1.00000   1.00000        11\n",
      "         717    0.87500   1.00000   0.93333         7\n",
      "         718    1.00000   0.50000   0.66667         2\n",
      "         719    1.00000   1.00000   1.00000         3\n",
      "         720    1.00000   1.00000   1.00000         7\n",
      "         721    1.00000   1.00000   1.00000         2\n",
      "         722    1.00000   0.75000   0.85714         4\n",
      "         723    0.75000   1.00000   0.85714         3\n",
      "         724    0.00000   0.00000   0.00000         2\n",
      "         725    1.00000   1.00000   1.00000         3\n",
      "         726    1.00000   1.00000   1.00000         2\n",
      "         727    1.00000   1.00000   1.00000         2\n",
      "         728    1.00000   1.00000   1.00000         3\n",
      "         729    1.00000   1.00000   1.00000         2\n",
      "         730    1.00000   0.50000   0.66667         2\n",
      "         731    1.00000   1.00000   1.00000         3\n",
      "         732    0.66667   0.66667   0.66667         3\n",
      "         733    1.00000   1.00000   1.00000         2\n",
      "         734    1.00000   1.00000   1.00000         2\n",
      "         735    1.00000   1.00000   1.00000         3\n",
      "         736    0.66667   1.00000   0.80000         2\n",
      "         737    1.00000   0.75000   0.85714         4\n",
      "         738    1.00000   1.00000   1.00000         4\n",
      "         739    1.00000   1.00000   1.00000         5\n",
      "         740    1.00000   1.00000   1.00000         3\n",
      "         741    1.00000   0.80000   0.88889         5\n",
      "         742    0.80000   0.80000   0.80000         5\n",
      "         743    1.00000   1.00000   1.00000         8\n",
      "         744    1.00000   1.00000   1.00000         4\n",
      "         745    1.00000   1.00000   1.00000         4\n",
      "         746    0.80000   0.80000   0.80000         5\n",
      "         747    1.00000   1.00000   1.00000         2\n",
      "         748    1.00000   1.00000   1.00000         3\n",
      "         749    1.00000   1.00000   1.00000         2\n",
      "         750    1.00000   1.00000   1.00000         4\n",
      "         751    1.00000   0.66667   0.80000         3\n",
      "         752    1.00000   1.00000   1.00000         3\n",
      "         753    1.00000   1.00000   1.00000         4\n",
      "         754    1.00000   1.00000   1.00000         3\n",
      "         755    1.00000   1.00000   1.00000         6\n",
      "         756    1.00000   1.00000   1.00000         2\n",
      "         757    0.75000   1.00000   0.85714         3\n",
      "         758    1.00000   0.50000   0.66667         2\n",
      "         759    1.00000   0.66667   0.80000         3\n",
      "         760    1.00000   1.00000   1.00000         4\n",
      "         761    1.00000   1.00000   1.00000         3\n",
      "         762    1.00000   1.00000   1.00000         3\n",
      "         763    1.00000   1.00000   1.00000         7\n",
      "         764    1.00000   1.00000   1.00000         8\n",
      "         765    1.00000   0.75000   0.85714         8\n",
      "         766    1.00000   1.00000   1.00000         3\n",
      "         767    1.00000   1.00000   1.00000         3\n",
      "         768    0.66667   0.50000   0.57143         4\n",
      "         769    0.75000   0.75000   0.75000         4\n",
      "         770    1.00000   1.00000   1.00000         2\n",
      "         771    1.00000   1.00000   1.00000         2\n",
      "         772    1.00000   1.00000   1.00000         2\n",
      "         773    1.00000   1.00000   1.00000         2\n",
      "         774    1.00000   1.00000   1.00000         4\n",
      "         775    1.00000   1.00000   1.00000         2\n",
      "         776    1.00000   1.00000   1.00000         3\n",
      "         777    1.00000   1.00000   1.00000         3\n",
      "         778    1.00000   1.00000   1.00000         3\n",
      "         779    0.33333   0.50000   0.40000         2\n",
      "         780    0.80000   1.00000   0.88889         4\n",
      "         781    1.00000   1.00000   1.00000         7\n",
      "         782    1.00000   1.00000   1.00000         5\n",
      "         783    1.00000   1.00000   1.00000         7\n",
      "         784    1.00000   1.00000   1.00000         6\n",
      "         785    1.00000   1.00000   1.00000         3\n",
      "         786    1.00000   1.00000   1.00000         6\n",
      "         787    1.00000   1.00000   1.00000         2\n",
      "         788    1.00000   0.66667   0.80000         3\n",
      "         789    1.00000   1.00000   1.00000         2\n",
      "         790    1.00000   0.75000   0.85714         4\n",
      "         791    1.00000   1.00000   1.00000         2\n",
      "         792    1.00000   1.00000   1.00000         2\n",
      "         793    1.00000   1.00000   1.00000         2\n",
      "         794    1.00000   1.00000   1.00000         4\n",
      "         795    1.00000   1.00000   1.00000         2\n",
      "         796    0.75000   1.00000   0.85714         3\n",
      "         797    0.80000   0.80000   0.80000         5\n",
      "         798    1.00000   1.00000   1.00000         4\n",
      "         799    1.00000   1.00000   1.00000         5\n",
      "         800    1.00000   1.00000   1.00000         2\n",
      "         801    1.00000   0.33333   0.50000         3\n",
      "         802    1.00000   1.00000   1.00000         3\n",
      "         803    1.00000   1.00000   1.00000         3\n",
      "         804    1.00000   1.00000   1.00000         2\n",
      "         805    1.00000   0.75000   0.85714         4\n",
      "         806    1.00000   1.00000   1.00000         3\n",
      "         807    1.00000   0.66667   0.80000         3\n",
      "         808    1.00000   1.00000   1.00000         2\n",
      "         809    1.00000   1.00000   1.00000         2\n",
      "         810    1.00000   1.00000   1.00000         4\n",
      "         811    0.66667   0.80000   0.72727         5\n",
      "         812    1.00000   1.00000   1.00000         4\n",
      "         813    0.75000   1.00000   0.85714         3\n",
      "         814    1.00000   1.00000   1.00000         2\n",
      "         815    1.00000   1.00000   1.00000         5\n",
      "         816    1.00000   1.00000   1.00000         2\n",
      "         817    1.00000   1.00000   1.00000         2\n",
      "         818    0.80000   1.00000   0.88889         4\n",
      "         819    1.00000   0.50000   0.66667         2\n",
      "         820    1.00000   1.00000   1.00000         2\n",
      "         821    0.50000   0.50000   0.50000         2\n",
      "         822    1.00000   1.00000   1.00000         2\n",
      "         823    1.00000   1.00000   1.00000         2\n",
      "         824    1.00000   0.66667   0.80000         3\n",
      "         825    1.00000   1.00000   1.00000         3\n",
      "         826    1.00000   1.00000   1.00000         2\n",
      "         827    0.66667   1.00000   0.80000         2\n",
      "         828    1.00000   1.00000   1.00000         7\n",
      "         829    1.00000   0.75000   0.85714         4\n",
      "         830    1.00000   1.00000   1.00000         2\n",
      "         831    0.85714   1.00000   0.92308         6\n",
      "         832    1.00000   0.75000   0.85714         4\n",
      "         833    1.00000   0.66667   0.80000         3\n",
      "         834    1.00000   1.00000   1.00000         2\n",
      "         835    0.75000   1.00000   0.85714         3\n",
      "         836    1.00000   1.00000   1.00000         2\n",
      "         837    1.00000   1.00000   1.00000        12\n",
      "         838    0.62500   1.00000   0.76923         5\n",
      "         839    0.66667   1.00000   0.80000         2\n",
      "         840    1.00000   0.75000   0.85714         4\n",
      "         841    1.00000   1.00000   1.00000         3\n",
      "         842    1.00000   1.00000   1.00000         5\n",
      "         843    1.00000   1.00000   1.00000         3\n",
      "         844    1.00000   1.00000   1.00000         6\n",
      "         845    0.66667   1.00000   0.80000         2\n",
      "         846    1.00000   1.00000   1.00000         2\n",
      "         847    1.00000   1.00000   1.00000         2\n",
      "         848    1.00000   1.00000   1.00000         3\n",
      "         849    1.00000   0.33333   0.50000         3\n",
      "         850    0.75000   1.00000   0.85714         3\n",
      "         851    1.00000   0.50000   0.66667         2\n",
      "         852    1.00000   1.00000   1.00000         6\n",
      "         853    1.00000   1.00000   1.00000         4\n",
      "         854    1.00000   1.00000   1.00000        10\n",
      "         855    1.00000   1.00000   1.00000         2\n",
      "         856    1.00000   1.00000   1.00000         2\n",
      "         857    0.80000   1.00000   0.88889         4\n",
      "         858    0.80000   1.00000   0.88889         4\n",
      "         859    1.00000   0.66667   0.80000         3\n",
      "         860    1.00000   1.00000   1.00000         2\n",
      "         861    1.00000   1.00000   1.00000         9\n",
      "         862    1.00000   1.00000   1.00000         2\n",
      "         863    1.00000   1.00000   1.00000         2\n",
      "         864    1.00000   1.00000   1.00000         3\n",
      "         865    1.00000   1.00000   1.00000         4\n",
      "         866    1.00000   1.00000   1.00000         3\n",
      "         867    1.00000   1.00000   1.00000         2\n",
      "         868    0.87500   1.00000   0.93333         7\n",
      "         869    0.66667   1.00000   0.80000         2\n",
      "         870    1.00000   1.00000   1.00000         2\n",
      "         871    1.00000   1.00000   1.00000         7\n",
      "         872    1.00000   0.50000   0.66667         2\n",
      "         873    1.00000   1.00000   1.00000         2\n",
      "         874    1.00000   1.00000   1.00000         2\n",
      "         875    0.66667   0.66667   0.66667         3\n",
      "         876    1.00000   1.00000   1.00000         3\n",
      "         877    1.00000   1.00000   1.00000         2\n",
      "         878    1.00000   0.75000   0.85714         4\n",
      "         879    1.00000   1.00000   1.00000         3\n",
      "         880    0.66667   1.00000   0.80000         2\n",
      "         881    0.00000   0.00000   0.00000         2\n",
      "         882    1.00000   1.00000   1.00000         3\n",
      "         883    1.00000   1.00000   1.00000         2\n",
      "         884    1.00000   1.00000   1.00000         4\n",
      "         885    1.00000   1.00000   1.00000         2\n",
      "         886    1.00000   1.00000   1.00000         8\n",
      "         887    1.00000   1.00000   1.00000        10\n",
      "         888    1.00000   1.00000   1.00000         2\n",
      "         889    0.66667   1.00000   0.80000         2\n",
      "         890    1.00000   1.00000   1.00000         3\n",
      "         891    1.00000   0.50000   0.66667         2\n",
      "         892    1.00000   1.00000   1.00000         6\n",
      "         893    0.66667   1.00000   0.80000         2\n",
      "         894    1.00000   1.00000   1.00000         3\n",
      "         895    1.00000   0.50000   0.66667         2\n",
      "         896    1.00000   1.00000   1.00000         3\n",
      "         897    1.00000   1.00000   1.00000         2\n",
      "         898    0.50000   0.66667   0.57143         3\n",
      "         899    0.75000   1.00000   0.85714         3\n",
      "         900    1.00000   1.00000   1.00000         3\n",
      "         901    1.00000   1.00000   1.00000         3\n",
      "         902    1.00000   1.00000   1.00000         3\n",
      "         903    1.00000   1.00000   1.00000         3\n",
      "         904    1.00000   1.00000   1.00000         4\n",
      "         905    0.80000   1.00000   0.88889         4\n",
      "         906    1.00000   1.00000   1.00000         3\n",
      "         907    1.00000   1.00000   1.00000         5\n",
      "         908    1.00000   1.00000   1.00000         3\n",
      "         909    1.00000   1.00000   1.00000         2\n",
      "         910    1.00000   1.00000   1.00000         3\n",
      "         911    1.00000   1.00000   1.00000         4\n",
      "         912    1.00000   1.00000   1.00000         2\n",
      "         913    1.00000   1.00000   1.00000         5\n",
      "         914    1.00000   0.75000   0.85714         4\n",
      "         915    0.66667   0.66667   0.66667         3\n",
      "         916    1.00000   1.00000   1.00000         3\n",
      "         917    1.00000   1.00000   1.00000         2\n",
      "         918    1.00000   1.00000   1.00000         6\n",
      "         919    0.66667   1.00000   0.80000         2\n",
      "         920    1.00000   1.00000   1.00000         3\n",
      "         921    1.00000   0.66667   0.80000         3\n",
      "         922    1.00000   1.00000   1.00000         4\n",
      "         923    1.00000   1.00000   1.00000         3\n",
      "         924    1.00000   0.66667   0.80000         3\n",
      "         925    1.00000   0.66667   0.80000         3\n",
      "         926    0.00000   0.00000   0.00000         2\n",
      "         927    1.00000   0.50000   0.66667         2\n",
      "         928    1.00000   1.00000   1.00000         7\n",
      "         929    0.66667   0.66667   0.66667         3\n",
      "         930    1.00000   1.00000   1.00000         2\n",
      "         931    0.75000   1.00000   0.85714         3\n",
      "         932    1.00000   0.60000   0.75000         5\n",
      "         933    1.00000   1.00000   1.00000         2\n",
      "         934    0.66667   1.00000   0.80000         2\n",
      "         935    1.00000   1.00000   1.00000         2\n",
      "         936    1.00000   1.00000   1.00000         3\n",
      "         937    1.00000   1.00000   1.00000         3\n",
      "         938    1.00000   1.00000   1.00000         2\n",
      "         939    1.00000   1.00000   1.00000         3\n",
      "         940    1.00000   1.00000   1.00000         3\n",
      "         941    1.00000   1.00000   1.00000         3\n",
      "         942    0.50000   0.50000   0.50000         2\n",
      "         943    0.66667   1.00000   0.80000         2\n",
      "         944    1.00000   1.00000   1.00000         2\n",
      "         945    1.00000   1.00000   1.00000         3\n",
      "         946    1.00000   1.00000   1.00000         4\n",
      "         947    1.00000   0.50000   0.66667         2\n",
      "         948    1.00000   1.00000   1.00000         4\n",
      "         949    1.00000   1.00000   1.00000         2\n",
      "         950    1.00000   1.00000   1.00000         3\n",
      "         951    0.75000   1.00000   0.85714         3\n",
      "         952    1.00000   1.00000   1.00000         3\n",
      "         953    1.00000   1.00000   1.00000         2\n",
      "         954    0.50000   0.33333   0.40000         3\n",
      "         955    0.00000   0.00000   0.00000         2\n",
      "         956    1.00000   0.50000   0.66667         2\n",
      "         957    1.00000   1.00000   1.00000         2\n",
      "         958    0.00000   0.00000   0.00000         2\n",
      "         959    1.00000   1.00000   1.00000         2\n",
      "         960    0.00000   0.00000   0.00000         2\n",
      "         961    1.00000   1.00000   1.00000         3\n",
      "         962    1.00000   1.00000   1.00000         4\n",
      "         963    1.00000   1.00000   1.00000         5\n",
      "         964    0.66667   1.00000   0.80000         2\n",
      "         965    1.00000   1.00000   1.00000         3\n",
      "         966    1.00000   1.00000   1.00000         3\n",
      "         967    1.00000   1.00000   1.00000         2\n",
      "         968    1.00000   1.00000   1.00000         2\n",
      "         969    1.00000   1.00000   1.00000         2\n",
      "         970    1.00000   1.00000   1.00000         7\n",
      "         971    1.00000   1.00000   1.00000         3\n",
      "         972    0.75000   0.75000   0.75000         4\n",
      "         973    1.00000   1.00000   1.00000         3\n",
      "         974    1.00000   1.00000   1.00000         2\n",
      "         975    1.00000   1.00000   1.00000         2\n",
      "         976    1.00000   1.00000   1.00000         3\n",
      "         977    0.66667   1.00000   0.80000         2\n",
      "         978    0.00000   0.00000   0.00000         2\n",
      "         979    1.00000   1.00000   1.00000         4\n",
      "         980    0.50000   0.50000   0.50000         2\n",
      "         981    1.00000   0.50000   0.66667         2\n",
      "         982    1.00000   1.00000   1.00000         2\n",
      "         983    1.00000   1.00000   1.00000         2\n",
      "         984    0.33333   0.50000   0.40000         2\n",
      "         985    0.66667   1.00000   0.80000         2\n",
      "         986    0.66667   1.00000   0.80000         2\n",
      "         987    1.00000   0.50000   0.66667         2\n",
      "         988    1.00000   1.00000   1.00000         2\n",
      "         989    0.80000   0.80000   0.80000         5\n",
      "         990    1.00000   1.00000   1.00000         2\n",
      "         991    1.00000   1.00000   1.00000         4\n",
      "         992    1.00000   1.00000   1.00000         3\n",
      "         993    1.00000   0.66667   0.80000         3\n",
      "         994    0.90000   1.00000   0.94737         9\n",
      "         995    1.00000   1.00000   1.00000         3\n",
      "         996    0.66667   1.00000   0.80000         2\n",
      "         997    1.00000   1.00000   1.00000         2\n",
      "         998    0.66667   1.00000   0.80000         2\n",
      "         999    1.00000   1.00000   1.00000         4\n",
      "        1000    1.00000   1.00000   1.00000         3\n",
      "        1001    1.00000   1.00000   1.00000         2\n",
      "        1002    1.00000   1.00000   1.00000         3\n",
      "        1003    1.00000   1.00000   1.00000         2\n",
      "        1004    1.00000   0.75000   0.85714         4\n",
      "        1005    1.00000   0.75000   0.85714         4\n",
      "        1006    1.00000   1.00000   1.00000         3\n",
      "        1007    1.00000   1.00000   1.00000         2\n",
      "        1008    1.00000   1.00000   1.00000         2\n",
      "        1009    0.66667   1.00000   0.80000         2\n",
      "        1010    1.00000   1.00000   1.00000         2\n",
      "        1011    1.00000   0.50000   0.66667         2\n",
      "        1012    1.00000   1.00000   1.00000         2\n",
      "        1013    1.00000   0.50000   0.66667         2\n",
      "        1014    0.50000   0.50000   0.50000         2\n",
      "        1015    1.00000   1.00000   1.00000         2\n",
      "        1016    1.00000   1.00000   1.00000         3\n",
      "        1017    1.00000   1.00000   1.00000         2\n",
      "        1018    1.00000   1.00000   1.00000         3\n",
      "        1019    1.00000   1.00000   1.00000         2\n",
      "        1020    1.00000   1.00000   1.00000         4\n",
      "        1021    0.00000   0.00000   0.00000         2\n",
      "        1022    1.00000   0.50000   0.66667         2\n",
      "        1023    1.00000   1.00000   1.00000         2\n",
      "        1024    1.00000   1.00000   1.00000         2\n",
      "        1025    1.00000   1.00000   1.00000         3\n",
      "        1026    1.00000   1.00000   1.00000         2\n",
      "        1027    1.00000   1.00000   1.00000         2\n",
      "        1028    1.00000   0.75000   0.85714         4\n",
      "        1029    1.00000   1.00000   1.00000         2\n",
      "        1030    1.00000   0.80000   0.88889         5\n",
      "        1031    1.00000   1.00000   1.00000         2\n",
      "        1032    1.00000   1.00000   1.00000         2\n",
      "        1033    1.00000   1.00000   1.00000         2\n",
      "        1034    1.00000   1.00000   1.00000         2\n",
      "        1035    1.00000   0.50000   0.66667         2\n",
      "        1036    1.00000   0.50000   0.66667         2\n",
      "        1037    0.66667   1.00000   0.80000         2\n",
      "        1038    1.00000   1.00000   1.00000         2\n",
      "        1039    1.00000   0.50000   0.66667         2\n",
      "        1040    0.60000   1.00000   0.75000         3\n",
      "        1041    1.00000   1.00000   1.00000         2\n",
      "        1042    1.00000   0.50000   0.66667         2\n",
      "        1043    1.00000   1.00000   1.00000         2\n",
      "        1044    0.66667   1.00000   0.80000         2\n",
      "        1045    0.50000   0.50000   0.50000         2\n",
      "        1046    1.00000   1.00000   1.00000         2\n",
      "        1047    1.00000   1.00000   1.00000         2\n",
      "        1048    1.00000   0.50000   0.66667         2\n",
      "        1049    1.00000   0.50000   0.66667         2\n",
      "        1050    1.00000   0.66667   0.80000         3\n",
      "        1051    1.00000   1.00000   1.00000         2\n",
      "        1052    1.00000   0.50000   0.66667         2\n",
      "        1053    1.00000   1.00000   1.00000         2\n",
      "        1054    1.00000   1.00000   1.00000         2\n",
      "        1055    1.00000   0.66667   0.80000         3\n",
      "        1056    1.00000   1.00000   1.00000         2\n",
      "        1057    1.00000   1.00000   1.00000         2\n",
      "        1058    1.00000   1.00000   1.00000         2\n",
      "        1059    1.00000   1.00000   1.00000         2\n",
      "        1060    1.00000   0.50000   0.66667         2\n",
      "        1061    1.00000   1.00000   1.00000         2\n",
      "        1062    0.00000   0.00000   0.00000         2\n",
      "        1063    1.00000   1.00000   1.00000         2\n",
      "        1064    0.50000   0.50000   0.50000         2\n",
      "        1065    1.00000   1.00000   1.00000         2\n",
      "        1066    1.00000   0.50000   0.66667         2\n",
      "        1067    0.66667   1.00000   0.80000         2\n",
      "        1068    1.00000   1.00000   1.00000         3\n",
      "        1069    1.00000   1.00000   1.00000         2\n",
      "        1070    1.00000   1.00000   1.00000         2\n",
      "        1071    1.00000   1.00000   1.00000         2\n",
      "        1072    0.66667   1.00000   0.80000         2\n",
      "        1073    1.00000   1.00000   1.00000         2\n",
      "        1074    1.00000   1.00000   1.00000         2\n",
      "        1075    1.00000   1.00000   1.00000         3\n",
      "        1076    1.00000   1.00000   1.00000         2\n",
      "        1077    0.00000   0.00000   0.00000         2\n",
      "        1078    1.00000   1.00000   1.00000         2\n",
      "        1079    1.00000   1.00000   1.00000         2\n",
      "\n",
      "    accuracy                        0.95867     10138\n",
      "   macro avg    0.93551   0.91715   0.91752     10138\n",
      "weighted avg    0.96185   0.95867   0.95711     10138\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\shine_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\user\\anaconda3\\envs\\shine_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\user\\anaconda3\\envs\\shine_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3i2aPPz21cJ"
   },
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "HcUfHVOl21LZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25344, 64, 64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_dataset = np.load('/content/drive/MyDrive/Colab Notebooks/kuzushiji-ml-it-kmitl-2020/test-images.npy')\n",
    "test_dataset = np.load('test-images.npy')\n",
    "test_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ต้องเตรียมข้อมูลให้เหมือนกับชุด Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "V8c-tz66Dhez"
   },
   "outputs": [],
   "source": [
    "X_test_dataset = test_dataset.reshape(-1, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "rfOwTdrcY9M7"
   },
   "outputs": [],
   "source": [
    "X_test_dataset = X_test_dataset / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "e1TCO3F3Dhe1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013619560309154876"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_X_test_dataset = normalize(X_test_dataset)\n",
    "np.std(normalized_X_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "7sw7dA7sDhe2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25344, 64, 64, 1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_norm_test_dataset = normalized_X_test_dataset.reshape(test_dataset.shape[0],test_dataset.shape[1],test_dataset.shape[2],1)\n",
    "X_norm_test_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "stWNYADPd0ZH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([412,  20, 241, 185, 557, 984, 915,  79, 332,  28], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(model.predict(X_norm_test_dataset), axis=-1)\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AA-cBgu1mlyN"
   },
   "source": [
    "## Save the prediction \n",
    "นำผลทำนายมาเซฟไว้ในไฟล์ csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "ZG7NYXVeNC_V"
   },
   "outputs": [],
   "source": [
    "index = np.arange(1, len(y_pred)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "AJMMXOeQMJnp"
   },
   "outputs": [],
   "source": [
    "d = {'ImageId': index, 'ClassId': y_pred}\n",
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "gJvxpZ1AnL7s"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>ClassId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId  ClassId\n",
       "0        1      412\n",
       "1        2       20\n",
       "2        3      241\n",
       "3        4      185\n",
       "4        5      557\n",
       "5        6      984\n",
       "6        7      915\n",
       "7        8       79\n",
       "8        9      332\n",
       "9       10       28"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "VTB-M9WonR0y"
   },
   "outputs": [],
   "source": [
    "# df.to_csv('/content/drive/MyDrive/Colab Notebooks/kuzushiji-ml-it-kmitl-2020/task1_CNN_predict_5.csv', index=False)\n",
    "df.to_csv('predict_CNN.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFtRrCdQnc4f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "task1_CNN_test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:shine_env]",
   "language": "python",
   "name": "conda-env-shine_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
